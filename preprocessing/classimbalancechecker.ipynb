{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalancer Checker Script\n",
    "\n",
    "The class imbalance checker script will serve as a diagnostic tool that:\n",
    "\n",
    "- Confirms that the slicing process has not unintentionally altered the dataset (such as by adding new defect types).\n",
    "  \n",
    "- Identifies any class imbalances in the original dataset that may require balancing measures.\n",
    "  \n",
    "- Highlights any changes in the distribution of bounding box sizes due to slicing, allowing you to see if the slicing process fragments large defects excessively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details open>\n",
    "<summary>Why Class Imbalance Matters in ML?</summary>\n",
    "<br>\n",
    "When training a model, we want it to identify all defect types, regardless of their frequency in the dataset. If some defect types are rare, the model might:\n",
    "\n",
    "- **Predict only the common defects**: It might say “crack” for every image, missing other defects.\n",
    "  \n",
    "- **Miss rare but important defects**: Some rare defects could be critical, and we want the model to catch those as well.\n",
    "\n",
    "In this example:\n",
    "\n",
    "Each defect type (e.g., line_crack, particle_material) shows up with a certain frequency in both the original and sliced data.\n",
    "If there are discrepancies between the counts in the original and sliced data, it could indicate that some classes became more or less frequent after slicing. \n",
    "E.g. if residue_stain had a count of 4 in the original data but became 0 in the sliced data, this would mean the slicing process didn’t preserve that defect type well.\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Step                          | Output                        | Purpose                                                        |\n",
    "|-------------------------------|-------------------------------|----------------------------------------------------------------|\n",
    "| 1                             | Loaded datasets               | Verify data structure and integrity                            |\n",
    "| 2                             | Defect types in each dataset  | Identify unexpected defect types in sliced data               |\n",
    "| 3                             | Total class counts            | Compare overall defect counts before and after slicing         |\n",
    "| 4                             | Size-based classification     | Analyze if slicing affects size distribution of defects       |\n",
    "| 5                             | Final summary table           | Comprehensive view of class imbalance and size distribution, guiding augmentation and balancing decisions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Step                     | Action                                           | Goal                                                      |\n",
    "|--------------------------|--------------------------------------------------|-----------------------------------------------------------|\n",
    "| 1. Count Each Defect     | Check how many times each defect type appears.   | Identify if any defect type is much rarer than others.    |\n",
    "| 2. Apply Augmentation    | Create new variations of rare defects.           | Increase the number of examples for rare classes.         |\n",
    "| 3. Use Class Weights     | Assign higher importance to rare classes.        | Encourage the model to focus more on rare defects.        |\n",
    "| 4. Balance the Dataset   | Use sampling techniques to even class counts.    | Ensure the model sees all defect types fairly often.      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in e:\\github_repos\\mitl_detect_inspection\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\github_repos\\mitl_detect_inspection\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in e:\\github_repos\\mitl_detect_inspection\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 11.6/11.6 MB 66.0 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas; pycocotools; matplotlib; pandas; numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_imbalance_check.py\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import os.path\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/coco_json_files/cassette1_train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m coco_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcassette1_train\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Change file name of original coco json file\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the original and sliced annotation files\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/coco_json_files/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcoco_file_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# . in script, .. in notebook\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     original_annotations \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/coco_json_files/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoco_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_sliced_coco.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# . in script, .. in notebook\u001b[39;00m\n",
      "File \u001b[1;32me:\\GitHub_Repos\\mitl_detect_inspection\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/coco_json_files/cassette1_train.json'"
     ]
    }
   ],
   "source": [
    "coco_file_name = 'cassette1_train' # Change file name of original coco json file\n",
    "\n",
    "# Load the original and sliced annotation files\n",
    "with open(f\"../data/coco/{coco_file_name}.json\") as f:  # . in script, .. in notebook\n",
    "    original_annotations = json.load(f)\n",
    "\n",
    "with open(f\"../data/coco/{coco_file_name}_sliced_coco.json\") as f:  # . in script, .. in notebook\n",
    "    sliced_annotations = json.load(f)\n",
    "\n",
    "# Use COCO objects specifically for counting bounding boxes\n",
    "coco_original = COCO(f\"../data/coco/{coco_file_name}.json\")\n",
    "coco_sliced = COCO(f\"../data/coco/{coco_file_name}_sliced_coco.json\")\n",
    "\n",
    "print(original_annotations)\n",
    "print(sliced_annotations)\n",
    "\n",
    "# Inspect the structure of both datasets to ensure they are loaded correctly\n",
    "print(\"\\nKeys in original dataset:\", original_annotations.keys())\n",
    "print(\"Keys in sliced dataset:\", sliced_annotations.keys())\n",
    "print(\"Sample data from original dataset:\", original_annotations['annotations'][:2])\n",
    "print(\"Sample data from sliced dataset:\", sliced_annotations['annotations'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 246, 7: 181, 1: 34, 5: 34, 9: 24, 2: 15, 4: 13, 0: 1, 3: 0, 8: 0, 10: 0})\n",
      "Counter({6: 423, 7: 312, 9: 53, 1: 51, 5: 50, 2: 27, 4: 21, 0: 2, 3: 0, 8: 0, 10: 0})\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate class counts by using COCO API\n",
    "def calculate_class_counts(coco_obj):\n",
    "    category_ids = coco_obj.getCatIds()  # Get all category IDs in the dataset\n",
    "    bbox_counts = Counter()\n",
    "    \n",
    "    for cat_id in category_ids:\n",
    "        # Get all annotation IDs for each category ID and count them\n",
    "        ann_ids = coco_obj.getAnnIds(catIds=[cat_id])\n",
    "        bbox_counts[cat_id] = len(ann_ids)\n",
    "    \n",
    "    return bbox_counts\n",
    "\n",
    "\n",
    "# Calculate total counts for original and sliced datasets\n",
    "original_counts = calculate_class_counts(coco_original)\n",
    "sliced_counts = calculate_class_counts(coco_sliced)\n",
    "\n",
    "print(original_counts)\n",
    "print(sliced_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Mapping:  {0: 'blocked_valve', 1: 'bubble', 2: 'chip_crack', 3: 'excessive_flash', 4: 'improper_welding', 5: 'light_stain', 6: 'line_crack', 7: 'particle_material', 8: 'residue_stain', 9: 'unknown', 10: 'welding_blob'}\n",
      "Bounding Box Counts in Original Dataset: {'blocked_valve': 1, 'bubble': 34, 'chip_crack': 15, 'excessive_flash': 0, 'improper_welding': 13, 'light_stain': 34, 'line_crack': 246, 'particle_material': 181, 'residue_stain': 0, 'unknown': 24, 'welding_blob': 0}\n",
      "Bounding Box Counts in Sliced Dataset: {'blocked_valve': 2, 'bubble': 51, 'chip_crack': 27, 'excessive_flash': 0, 'improper_welding': 21, 'light_stain': 50, 'line_crack': 423, 'particle_material': 312, 'residue_stain': 0, 'unknown': 53, 'welding_blob': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Map category IDs to names for readability\n",
    "category_mapping = {cat['id']: cat['name'] for cat in coco_original.loadCats(original_counts.keys())}\n",
    "original_counts_named = {category_mapping[k]: v for k, v in original_counts.items()}\n",
    "sliced_counts_named = {category_mapping.get(k, 'Unknown'): v for k, v in sliced_counts.items()}\n",
    "\n",
    "print(\"Category Mapping: \",category_mapping)\n",
    "print(\"Bounding Box Counts in Original Dataset:\", original_counts_named)\n",
    "print(\"Bounding Box Counts in Sliced Dataset:\", sliced_counts_named)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total bounding boxes in the original dataset: 548\n",
      "Total bounding boxes in the sliced dataset: 939\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of bounding boxes across all defect types\n",
    "total_original_bboxes = sum(original_counts.values())\n",
    "total_sliced_bboxes = sum(sliced_counts.values())\n",
    "\n",
    "print(f\"\\nTotal bounding boxes in the original dataset: {total_original_bboxes}\")\n",
    "print(f\"Total bounding boxes in the sliced dataset: {total_sliced_bboxes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defect types in the original dataset: {0, 1, 2, 4, 5, 6, 7, 9}\n",
      "Defect types in the sliced dataset: {0, 1, 2, 4, 5, 6, 7, 9}\n",
      "All defect types in the sliced dataset are also present in the original dataset.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check unique defect types in each dataset\n",
    "original_defect_types = {ann['category_id'] for ann in original_annotations['annotations']}\n",
    "sliced_defect_types = {ann['category_id'] for ann in sliced_annotations['annotations']}\n",
    "\n",
    "print(\"Defect types in the original dataset:\", original_defect_types)\n",
    "print(\"Defect types in the sliced dataset:\", sliced_defect_types)\n",
    "\n",
    "# Ensure that all defect types in the sliced dataset exist in the original dataset\n",
    "unexpected_defects = sliced_defect_types - original_defect_types\n",
    "if unexpected_defects:\n",
    "    print(\"Warning: These defect types appear in the sliced dataset but not in the original dataset:\", unexpected_defects)\n",
    "else:\n",
    "    print(\"All defect types in the sliced dataset are also present in the original dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Imbalance in Original Annotations:\n",
      "blocked_valve: 1\n",
      "bubble: 34\n",
      "chip_crack: 15\n",
      "excessive_flash: 0\n",
      "improper_welding: 13\n",
      "light_stain: 34\n",
      "line_crack: 246\n",
      "particle_material: 181\n",
      "residue_stain: 0\n",
      "unknown: 24\n",
      "welding_blob: 0\n",
      "\n",
      "Class Imbalance in Sliced Annotations:\n",
      "blocked_valve: 2\n",
      "bubble: 51\n",
      "chip_crack: 27\n",
      "excessive_flash: 0\n",
      "improper_welding: 21\n",
      "light_stain: 50\n",
      "line_crack: 423\n",
      "particle_material: 312\n",
      "residue_stain: 0\n",
      "unknown: 53\n",
      "welding_blob: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Class Imbalance in Original Annotations:\")\n",
    "for cat_id, count in original_counts.items():\n",
    "    print(f\"{category_mapping[cat_id]}: {count}\")\n",
    "\n",
    "print(\"\\nClass Imbalance in Sliced Annotations:\")\n",
    "for cat_id, count in sliced_counts.items():\n",
    "    print(f\"{category_mapping[cat_id]}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Imbalance Comparison\n",
      "      Defect Type  Original Count  Sliced Count\n",
      "    blocked_valve               1             2\n",
      "           bubble              34            51\n",
      "       chip_crack              15            27\n",
      "  excessive_flash               0             0\n",
      " improper_welding              13            21\n",
      "      light_stain              34            50\n",
      "       line_crack             246           423\n",
      "particle_material             181           312\n",
      "    residue_stain               0             0\n",
      "          unknown              24            53\n",
      "     welding_blob               0             0\n"
     ]
    }
   ],
   "source": [
    "# Convert counts into DataFrame format\n",
    "data = {\n",
    "    \"Defect Type\": [category_mapping[cat_id] for cat_id in original_counts.keys()],\n",
    "    \"Original Count\": [original_counts[cat_id] for cat_id in original_counts.keys()],\n",
    "    \"Sliced Count\": [sliced_counts.get(cat_id, 0) for cat_id in original_counts.keys()]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Class Imbalance Comparison\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "- **Class Imbalance Remains**: In both the original and sliced datasets, certain defects, like `line_crack` and `particle_material`, are **more** **common** than others, like `chip_crack` and `light_stain`.\n",
    "\n",
    "- **Increased Counts After Slicing**: For most classes (except `light_stain` and `chip_crack`), slicing increased the counts, which is good for adding examples. However, rare defects like `chip_crack` still have only 1 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding Box Size Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small Threshold:  184.98383589988535\n",
      "Medium Threshold:  872.8431891833333\n",
      "All Areas:  [11250.788755982963, 3684.9086114129277, 1729.6509808672772, 706.2389241390035, 536.200551158886, 956.4658480131437, 524.1239621688045, 127.2271400619153, 149.1001533907278, 57.86618380479451, 28.05633154172121, 19.63272858873214, 1771.2918776398874, 2486.0236879155777, 6605.720085032894, 225.5296437842936, 52.17205830644165, 634.823906021296, 488.32608155478863, 2097.582486678786, 110.25893696121001, 1194.215924463596, 165.09897573229276, 128.49490685429316, 3573.99412372357, 3529.6336383840285, 83.94520793686539, 71.80437153737263, 1194.2669232972185, 2209.8708494682955, 8257.767394851355, 1520.9111140458522, 5661.652589546959, 484.6091519332006, 261.978879143369, 608.8679088391576, 298.22101657427, 159.7101634148239, 2487.6613966990813, 128.59151956620022, 871.7347203495611, 252.50698387546726, 618.1394345547818, 75.25879292348442, 6160.859301859299, 2439.670589034275, 1372.0237741698409, 175.6425665972623, 840.6759078488474, 195.4007373121175, 126.70479727532555, 481.47822964622617, 83.05108079381847, 3521.8141423235893, 56.52087442912307, 63.3804371519947, 27.22365933136272, 27.163044493715518, 94.80493804565252, 1113.5722410120777, 1222.2134352571654, 1047.6115159347125, 118.54663585766133, 459.065799061051, 122.00049840910563, 257.32768898106536, 775.9332726951277, 6451.814623071576, 4270.172417806588, 8145.847406791062, 3250.9600394639315, 183.86825153432972, 9435.344486630192, 1238.0865489828145, 211.69042117439645, 514.7101383411685, 431.8484591957672, 319.3501210859251, 11017.579177465046, 4165.539109420811, 571.1676712534746, 65.1657327618939, 381.9620437789972, 1879.2327789851483, 29.29609192820268, 16.57954023389714, 1409.858205021461, 55.9651346196323, 172.51389850040948, 328.962669014579, 26.03398879702477, 35.104552096903205, 143.64384552372616, 40.568292781207774, 27.751306422959168, 33.918263405838644, 215.41311461212374, 97.3414640367962, 990.0593847569965, 1110.066582909346, 282.87410993056216, 56.33275559766667, 900.3757195004466, 906.7840238195217, 1042.1911786788512, 603.3301968299852, 529.317463279127, 642.8957043876513, 754.9425645364165, 197.72805826358837, 953.813782137882, 489.45707241286266, 47.601199211601596, 3128.9597991885657, 113.55992584693855, 11435.185893943242, 14593.475331318026, 14686.47659130488, 82.62137487657748, 494.65708503305996, 63.76438986754367, 35.1550236437419, 27.22455574764209, 19.293073369316055, 80.28010200613629, 187.33935232430693, 3629.448557420617, 1013.1560109236121, 142.53360955421468, 1268.2608659408459, 1261.4146534391923, 352.9023527313473, 281.6479793649152, 386.70739097601364, 790.9880319841346, 91.6194000807575, 3501.1699316577015, 1099.432800969132, 870.3843007672241, 26.72232502355619, 42.53757860892304, 46.62773039824389, 26.236869334950452, 75.30552244895051, 128.50692050317738, 147.24546441550865, 68.71455006056948, 19.63272858873522, 134.15697868968377, 1915.3470138243197, 31.017312138730468, 30.409129547777688, 320.667900282666, 130.8848572582263, 83.98445007403107, 760.7682328134774, 31.387722725205148, 52.35394290329413, 35.993335746014544, 61.295938660878384, 57.488555853534685, 1420.5779248862934, 1119.6695553566565, 2189.6563951734684, 306.9479572045457, 3167.4385527038717, 835.7126244673525, 500.4473891291101, 137.42910012114893, 197.21768466420204, 44.87770004099164, 88.1129118250121, 693.7045952878564, 2054.3917290343093, 153.91904733659655, 317.4580351317286, 435.1921503836133, 4787.318064147131, 52.63491051382972, 1309.8761710991914, 231.87050860924876, 11845.684608574933, 318.82194933772445, 521.7086443708109, 933.2787971522258, 220.27698317878878, 1358.3747296024997, 273.2295043158575, 403.84095994433403, 319.5911240340112, 123.18120769866026, 77.29016953641514, 161.65798115472555, 35.15502364374001, 78.35873156478021, 81.35233035506077, 1117.4603050303663, 78.53091435493666, 178.8573016070511, 346.58950789880925, 1150.1637002864786, 51.34659376278771, 233.62700162068987, 2909.640313224672, 280.69471256990613, 377.3974641564568, 35.13189524382635, 32.32297677047481, 508.2304841752653, 337.0916476672533, 661.2182319627069, 375.84655579702974, 885.455504751565, 100.95801661792034, 821.8527952568346, 166.28921114658172, 1935.7665848571635, 3267.3549164387623, 1995.470533758985, 603.7882071393757, 1615.9153971451303, 510.5227216255222, 5819.959026530967, 1031.6001967050447, 246.4636500613771, 10754.272114241947, 1361.1493332595255, 77.57514424913542, 204.0965104650036, 1677.4932182302914, 3168.5983011016933, 3702.9109557971847, 36.940544880537146, 186.05567185894853, 129.62437198581983, 121.24348586603931, 100.96028395694135, 51.38041917161902, 2437.538721754701, 21.04224865628879, 53.171973261153255, 286.1540622384058, 217.60519645584836, 221.72617385760145, 463.7410172184952, 510.1151189403466, 167.67495457280492, 518.1954548464242, 1151.313648776825, 479.2004441372188, 872.729607938977, 253.28900273091375, 265.0853480660208, 96.96893031141796, 3563.1198542780426, 202.81744409849316, 252.48944866825798, 76.38336262233257, 82.7486428408539, 35.362667880709274, 108.20976371493074, 878.4086701567785, 102.76083834323745, 1871.8960434096443, 279.67070853308644, 122.9196630899967, 660.3006793018233, 463.09280618321844, 59.58412588726246, 122.21508057945661, 46.8952943209454, 569.0914142042355, 770.4618867506986, 980.8984842509086, 503.0248637184021, 1303.073932680074, 113.9763368215016, 44.67969962841289, 1002.3539723746358, 6689.263080103537, 66.27133701650789, 1093.4770607723558, 5519.159785905887, 1025.1347444740754, 135.79264466192132, 101.13723013882807, 545.7817379579974, 309.2763181762036, 354.75812967269985, 2511.9305099335243, 146.4202431130599, 325.1950938309934, 68.46212501705212, 2377.4252505450177, 6438.860053559465, 852.0008765184493, 4149.745445631014, 451.0592875685923, 1486.3981310732645, 1101.0356526468577, 302.7848044778723, 2275.5886692023305, 411.7637609343929, 468.90897609819564, 1708.724866779552, 3564.713299369145, 2634.3815661567055, 788.9119596964516, 278.16451319667124, 577.3666786519026, 1194.108499008744, 3851.467948293551, 641.1247518657929, 2271.509790001162, 611.753049216704, 972.96804178977, 28.283954703191544, 231.2326303074014, 680.9648029866374, 315.15712608169616, 586.5338799122967, 1716.0305515148207, 327.92991207232365, 730.3251843442825, 221.3651853672128, 107.71972082870576, 92.33118928174163, 138.6353410858791, 346.3083770367533, 365.63150955571905, 301.6152183203675, 2861.035785210405, 189.01660539026997, 376.71125226953325, 323.1591624861182, 529.060258143343, 503.8742087814936, 205.61275388195057, 864.219931677151, 1008.2565869566765, 5687.0160338439455, 8948.865315515635, 19601.41027443236, 1545.4657306572276, 4074.3486614529097, 161.32390636489143, 1113.3829810372592, 1349.4394063278235, 852.3509787139244, 2128.5487780361755, 691.3235347467918, 2697.8126078979667, 3725.55074424008, 13822.126324415594, 605.1712108212065, 551.2946848060241, 1127.6482189214917, 942.0614723420625, 1563.503780076875, 903.7000502250726, 1928.9203723554012, 383.65626510584985, 635.1686969650542, 55.91644060767658, 194.98860436485043, 365.19605105957487, 243.979597471062, 462.27713205041476, 47.099194722688914, 46.1914475407181, 26.085432218539527, 721.644742411161, 1391.7434317928885, 163.48269783348775, 8554.232099703377, 2975.385078157735, 1880.8684244068404, 110.04260868256586, 167.60158690214996, 77.87548482322065, 220.08289189171379, 3999.700757775131, 4604.20417812833, 629.8227406841531, 207.07799217468065, 169.4274481429237, 82.82070328247076, 1458.5062540195722, 170.70531999293434, 1754.5062061915285, 1970.4454315689868, 62.02802420342247, 244.8474639609164, 390.3568139719437, 734.9888202252885, 1263.2620347622255, 41777.22390961886, 845.2371432590995, 7340.7008420002485, 2314.4245357955547, 497.17267805977593, 2931.604412007685, 683.6124323322105, 1194.3694528614212, 1371.1073118232007, 1377.61228041711, 42.13690118704899, 22.54793646865347, 49.979850360481066, 711.1615262656122, 674.519203508872, 160.2090920525459, 12569.554702554067, 648.6867876366599, 2608.292113969451, 161.11168100932673, 114.67481992958372, 108.94107893310576, 302.6141081474982, 481.72901265410013, 107.92174720037859, 156.87196825198933, 1400.7494686132575, 84.18088999036318, 149.87047877950081, 19.12506064060866, 76.92769170276308, 73.7446982369456, 329.69010729754046, 4315.285492322422, 563.8413107077173, 420.1584311469507, 1743.2889292324126, 66.1740676931798, 3116.6228477505306, 2191.9677726023097, 3114.120558948535, 40.90636188772108, 89.0382716264799, 523.6844196807859, 51.76348649701045, 112.98649640877056, 375.97891299875744, 700.3747343066043, 553.677106619166, 1950.1169699758884, 1374.613124509058, 2045.4243292694816, 760.116648864515, 2149.0775946193785, 118.71392192043339, 285.4221865601185, 93.14378531673152, 55.03950950534037, 87.09548756889576, 41.867525843173134, 78.24326672028342, 1129.7961551338442, 1152.182208981714, 45.41186050930316, 100.34046743838125, 870.0521866046199, 84.2738023741116, 685.7978491047925, 70.60078445451964, 59.336700760214924, 225.61791519055933, 1606.6051108094578, 5091.457044131575, 110.56800819656598, 207.38580197488062, 1317.3192494795069, 158.38943322922694, 671.8109233964178, 327.9077126101527, 1220.6564545640274, 1567.5588212583016, 5582.489218024842, 2013.8960126165273, 1358.4757476260436, 689.579309934879, 40.81124870428739, 1440.2719550462064, 333.7528594581262, 3155.3711713241782, 23.233443806771497, 71.90803422549058, 1925.8666210933477, 891.9602399280433, 26.933800456598398, 603.7486099223538, 1632.036224508611, 687.3801227721294, 791.258110629083, 2295.949199400439, 290.96403132246854, 180.28217253194452, 2397.237127632902, 432.8390548598679, 726.7066299716221, 2155.061040605481, 60.13361926459078, 4175.623823354017, 415.86497427712925, 7879.924412757136, 982.0961624978435, 7932.0885674048195, 407.0583457910841, 162.88424158501132, 217.37860182116972, 1463.682585595875, 355.5347798675234, 1704.2482382779692, 37.53094768786432, 422.3218060865453, 871.4466615230926, 35.15502364374001, 29.746558467780776, 2391.230990550703, 231.7749755499454, 116.86563036150261, 4419.4697872882025, 1197.9718555559582, 409.80201938782903, 40.2699721384552, 13588.010538156019, 103.85007050800168, 173.69470586168887, 1137.6592565481678, 14038.946605841958, 6140.176222512659, 734.1856086193069, 35.96163682437546, 5641.412262985178, 89.61359365747103, 106.18729054895702, 120.04526604195465, 1112.8867856023699, 580.8714597396172, 738.8785335112741, 837.9074013852417, 1939.8331817378019, 465.55996361708156, 58.15869365979231, 147.1894738542574, 1652.4274975315552, 1570.4889439349363, 73.45164321376949]\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate bounding box area\n",
    "def calculate_area(bbox):\n",
    "    return bbox[2] * bbox[3]  # width * height\n",
    "\n",
    "# Collect all bounding box areas from the original dataset\n",
    "all_areas = [calculate_area(ann['bbox']) for ann in original_annotations['annotations']]\n",
    "\n",
    "# Calculate dynamic thresholds based on percentiles\n",
    "small_threshold = np.percentile(all_areas, 33)\n",
    "medium_threshold = np.percentile(all_areas, 66)\n",
    "\n",
    "print(\"Small Threshold: \", small_threshold)\n",
    "print(\"Medium Threshold: \", medium_threshold)\n",
    "print(\"All Areas: \", all_areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Benefits of Dynamic Thresholds\n",
    "1. **Adaptability**: Since the thresholds are derived from the dataset, they better represent the natural distribution of bounding box sizes.\n",
    "2. **Scalability**: This approach automatically adjusts if the dataset grows or changes in defect types and bounding box sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorising dynamic threshold\n",
    "def categorize_by_dynamic_size(area, small_threshold, medium_threshold):\n",
    "    if area > medium_threshold:\n",
    "        return \"Large\"\n",
    "    elif area > small_threshold:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Small\"\n",
    "    \n",
    "# Prepare data for bounding box size analysis with dynamic thresholds\n",
    "def create_bbox_size_data(annotations, category_mapping, small_threshold, medium_threshold):\n",
    "    bbox_data = []\n",
    "    for ann in annotations['annotations']:\n",
    "        defect_type = category_mapping[ann['category_id']]\n",
    "        area = calculate_area(ann['bbox'])\n",
    "        size_category = categorize_by_dynamic_size(area, small_threshold, medium_threshold)\n",
    "        bbox_data.append({\n",
    "            \"Defect Type\": defect_type,\n",
    "            \"Bounding Box Area\": area,\n",
    "            \"Size Category\": size_category\n",
    "        })\n",
    "    return bbox_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped by Dynamic Bounding Box Size Category\n",
      "          Defect Type Size Category  Original Count  Sliced Count\n",
      "0       blocked_valve         Large               1             2\n",
      "1              bubble         Large               4             7\n",
      "2              bubble        Medium               8            12\n",
      "3              bubble         Small              22            32\n",
      "4          chip_crack         Large              10            20\n",
      "5          chip_crack        Medium               4             6\n",
      "6          chip_crack         Small               1             1\n",
      "7    improper_welding         Large               6             9\n",
      "8    improper_welding        Medium               4             7\n",
      "9    improper_welding         Small               3             5\n",
      "10        light_stain         Large              17            23\n",
      "11        light_stain        Medium              14            23\n",
      "12        light_stain         Small               3             4\n",
      "13         line_crack         Large             129           216\n",
      "14         line_crack        Medium             103           179\n",
      "15         line_crack         Small              14            28\n",
      "16  particle_material         Large               4             9\n",
      "17  particle_material        Medium              40            58\n",
      "18  particle_material         Small             137           245\n",
      "19            unknown         Large              15            32\n",
      "20            unknown        Medium               8            20\n",
      "21            unknown         Small               1             1\n"
     ]
    }
   ],
   "source": [
    "original_bbox_df = pd.DataFrame(original_bbox_data)\n",
    "sliced_bbox_df = pd.DataFrame(sliced_bbox_data)\n",
    "\n",
    "# Group by defect type and size category, counting occurrences\n",
    "grouped_original_bbox_df = original_bbox_df.groupby(['Defect Type', 'Size Category']).size().reset_index(name='Original Count')\n",
    "grouped_sliced_bbox_df = sliced_bbox_df.groupby(['Defect Type', 'Size Category']).size().reset_index(name='Sliced Count')\n",
    "\n",
    "# Merge original and sliced data on defect type and size category\n",
    "merged_bbox_df = pd.merge(grouped_original_bbox_df, grouped_sliced_bbox_df, on=['Defect Type', 'Size Category'], how='outer').fillna(0)\n",
    "\n",
    "# Converting the count to integers\n",
    "merged_bbox_df['Original Count'] = merged_bbox_df['Original Count'].astype(int)\n",
    "merged_bbox_df['Sliced Count'] = merged_bbox_df['Sliced Count'].astype(int)\n",
    "\n",
    "print(\"Grouped by Dynamic Bounding Box Size Category\")\n",
    "print(merged_bbox_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
    "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
    "\n",
    "    inter_width = max(0, xB - xA)\n",
    "    inter_height = max(0, yB - yA)\n",
    "    inter_area = inter_width * inter_height\n",
    "\n",
    "    boxA_area = boxA[2] * boxA[3]\n",
    "    boxB_area = boxB[2] * boxB[3]\n",
    "\n",
    "    iou = inter_area / float(boxA_area + boxB_area - inter_area)\n",
    "    return iou\n",
    "\n",
    "def remove_duplicate_bboxes(annotations, iou_threshold=0.7):\n",
    "    # Dictionary to keep unique annotations\n",
    "    unique_annotations = []\n",
    "    # Set to track which annotation IDs have been removed\n",
    "    removed_ids = set()\n",
    "    \n",
    "    # Group annotations by category_id\n",
    "    category_groups = defaultdict(list)\n",
    "    for ann in annotations:\n",
    "        category_groups[ann['category_id']].append(ann)\n",
    "\n",
    "    # Loop through each category's annotations to detect duplicates\n",
    "    for cat_id, bboxes in category_groups.items():\n",
    "        # Compare each pair of bounding boxes within the same category\n",
    "        for i in range(len(bboxes)):\n",
    "            for j in range(i + 1, len(bboxes)):\n",
    "                # Skip if either bounding box has already been marked as duplicate\n",
    "                if bboxes[i]['id'] in removed_ids or bboxes[j]['id'] in removed_ids:\n",
    "                    continue\n",
    "                \n",
    "                bboxA = bboxes[i]['bbox']\n",
    "                bboxB = bboxes[j]['bbox']\n",
    "                iou = calculate_iou(bboxA, bboxB)\n",
    "                \n",
    "                if iou > iou_threshold:\n",
    "                    # Mark the second bounding box as duplicate\n",
    "                    removed_ids.add(bboxes[j]['id'])\n",
    "\n",
    "    # Collect annotations that are not marked as duplicates\n",
    "    unique_annotations = [ann for ann in annotations if ann['id'] not in removed_ids]\n",
    "    \n",
    "    return unique_annotations, removed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bounding boxes before removing duplicates: 939\n",
      "Total bounding boxes after removing duplicates: 939\n",
      "Number of duplicates removed: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "unique_annotations, removed_ids = remove_duplicate_bboxes(sliced_annotations['annotations'], iou_threshold=0.7)\n",
    "\n",
    "# Print results\n",
    "print(f\"Total bounding boxes before removing duplicates: {len(sliced_annotations['annotations'])}\")\n",
    "print(f\"Total bounding boxes after removing duplicates: {len(unique_annotations)}\")\n",
    "print(f\"Number of duplicates removed: {len(removed_ids)}\")\n",
    "\n",
    "sliced_annotations['annotations'] = unique_annotations\n",
    "with open(\"../data/coco_json_files/coco_sliced_coco.json\", \"w\") as f:\n",
    "    json.dump(sliced_annotations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Imbalance After Deduplication\n",
      "         Defect Type  Bounding Box Count\n",
      "0         line_crack                 423\n",
      "1  particle_material                 312\n",
      "2            unknown                  53\n",
      "3             bubble                  51\n",
      "4        light_stain                  50\n",
      "5         chip_crack                  27\n",
      "6   improper_welding                  21\n",
      "7      blocked_valve                   2\n"
     ]
    }
   ],
   "source": [
    "def calculate_class_counts_deduped(annotations, category_mapping):\n",
    "    bbox_counts = Counter(ann['category_id'] for ann in annotations)\n",
    "    return {category_mapping[cat_id]: count for cat_id, count in bbox_counts.items()}\n",
    "\n",
    "# Calculate class counts on deduplicated annotations\n",
    "deduped_counts = calculate_class_counts_deduped(unique_annotations, category_mapping)\n",
    "\n",
    "# Display the class imbalance in a DataFrame format\n",
    "deduped_counts_df = pd.DataFrame(list(deduped_counts.items()), columns=['Defect Type', 'Bounding Box Count'])\n",
    "deduped_counts_df = deduped_counts_df.sort_values(by=\"Bounding Box Count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Class Imbalance After Deduplication\")\n",
    "print(deduped_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Imbalance Comparison\n",
      "           Defect Type  Original Count  Sliced Count  Deduplicated Sliced Count\n",
      "0          line_crack             246           423                        423\n",
      "1   particle_material             181           312                        312\n",
      "2              bubble              34            51                         51\n",
      "3         light_stain              34            50                         50\n",
      "4             unknown              24            53                         53\n",
      "5          chip_crack              15            27                         27\n",
      "6    improper_welding              13            21                         21\n",
      "7       blocked_valve               1             2                          2\n",
      "8     excessive_flash               0             0                          0\n",
      "9       residue_stain               0             0                          0\n",
      "10       welding_blob               0             0                          0\n"
     ]
    }
   ],
   "source": [
    "# Create bounding box data for original and sliced annotations\n",
    "original_bbox_data = create_bbox_size_data(original_annotations, category_mapping, small_threshold, medium_threshold)\n",
    "sliced_bbox_data = create_bbox_size_data(sliced_annotations, category_mapping, small_threshold, medium_threshold)\n",
    "\n",
    "# Remove duplicates from sliced dataset\n",
    "unique_annotations, removed_ids = remove_duplicate_bboxes(sliced_annotations['annotations'], iou_threshold=0.7)\n",
    "sliced_annotations['annotations'] = unique_annotations  # Overwrite sliced_annotations with deduped annotations\n",
    "\n",
    "# Calculate class counts after deduplication\n",
    "deduped_counts = calculate_class_counts_deduped(unique_annotations, category_mapping)\n",
    "\n",
    "# Prepare data for DataFrame display\n",
    "data = {\n",
    "    \"Defect Type\": [category_mapping[cat_id] for cat_id in original_counts.keys()],\n",
    "    \"Original Count\": [original_counts[cat_id] for cat_id in original_counts.keys()],\n",
    "    \"Sliced Count\": [sliced_counts.get(cat_id, 0) for cat_id in original_counts.keys()],\n",
    "    \"Deduplicated Sliced Count\": [deduped_counts.get(category_mapping[cat_id], 0) for cat_id in original_counts.keys()]\n",
    "}\n",
    "\n",
    "# Create and display DataFrame\n",
    "df = pd.DataFrame(data).sort_values(by=\"Original Count\", ascending=False).reset_index(drop=True)\n",
    "print(\"Class Imbalance Comparison\\n\", df)\n",
    "\n",
    "# Display the bounding box data for original and sliced images\n",
    "#print(\"\\nBounding Box Data for Original Dataset:\", original_bbox_data)\n",
    "#print(\"\\nBounding Box Data for Sliced Dataset (After Deduplication):\", sliced_bbox_data)\n",
    "\n",
    "# Optional: Save deduplicated annotations back to the original file\n",
    "with open(f\"../data/coco_json_files/{coco_file_name}_sliced_coco.json\", \"w\") as f:\n",
    "    json.dump(sliced_annotations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Imbalance by Size Category\n",
      "           Defect Type Size Category  Original Count  Sliced Count  \\\n",
      "0       blocked_valve         Large               1             2   \n",
      "1       blocked_valve        Medium               0             0   \n",
      "2       blocked_valve         Small               0             0   \n",
      "3              bubble         Large               4             7   \n",
      "4              bubble        Medium               8            12   \n",
      "5              bubble         Small              22            32   \n",
      "6          chip_crack         Large              10            20   \n",
      "7          chip_crack        Medium               4             6   \n",
      "8          chip_crack         Small               1             1   \n",
      "9     excessive_flash         Large               0             0   \n",
      "10    excessive_flash        Medium               0             0   \n",
      "11    excessive_flash         Small               0             0   \n",
      "12   improper_welding         Large               6             9   \n",
      "13   improper_welding        Medium               4             7   \n",
      "14   improper_welding         Small               3             5   \n",
      "15        light_stain         Large              17            23   \n",
      "16        light_stain        Medium              14            23   \n",
      "17        light_stain         Small               3             4   \n",
      "18         line_crack         Large             129           216   \n",
      "19         line_crack        Medium             103           179   \n",
      "20         line_crack         Small              14            28   \n",
      "21  particle_material         Large               4             9   \n",
      "22  particle_material        Medium              40            58   \n",
      "23  particle_material         Small             137           245   \n",
      "24      residue_stain         Large               0             0   \n",
      "25      residue_stain        Medium               0             0   \n",
      "26      residue_stain         Small               0             0   \n",
      "27            unknown         Large              15            32   \n",
      "28            unknown        Medium               8            20   \n",
      "29            unknown         Small               1             1   \n",
      "30       welding_blob         Large               0             0   \n",
      "31       welding_blob        Medium               0             0   \n",
      "32       welding_blob         Small               0             0   \n",
      "\n",
      "    Deduplicated Sliced Count  \n",
      "0                           2  \n",
      "1                           0  \n",
      "2                           0  \n",
      "3                           7  \n",
      "4                          12  \n",
      "5                          32  \n",
      "6                          20  \n",
      "7                           6  \n",
      "8                           1  \n",
      "9                           0  \n",
      "10                          0  \n",
      "11                          0  \n",
      "12                          9  \n",
      "13                          7  \n",
      "14                          5  \n",
      "15                         23  \n",
      "16                         23  \n",
      "17                          4  \n",
      "18                        216  \n",
      "19                        179  \n",
      "20                         28  \n",
      "21                          9  \n",
      "22                         58  \n",
      "23                        245  \n",
      "24                          0  \n",
      "25                          0  \n",
      "26                          0  \n",
      "27                         32  \n",
      "28                         20  \n",
      "29                          1  \n",
      "30                          0  \n",
      "31                          0  \n",
      "32                          0  \n"
     ]
    }
   ],
   "source": [
    "# Calculate size-category-based counts\n",
    "def calculate_size_category_counts(bbox_data):\n",
    "    size_category_counts = defaultdict(lambda: defaultdict(int))\n",
    "    for data in bbox_data:\n",
    "        defect_type = data[\"Defect Type\"]\n",
    "        size_category = data[\"Size Category\"]\n",
    "        size_category_counts[defect_type][size_category] += 1\n",
    "    return size_category_counts\n",
    "\n",
    "# Calculate counts for original, sliced, and deduplicated sliced datasets\n",
    "original_size_counts = calculate_size_category_counts(original_bbox_data)\n",
    "sliced_size_counts = calculate_size_category_counts(sliced_bbox_data)\n",
    "deduped_size_counts = calculate_size_category_counts(create_bbox_size_data(sliced_annotations, category_mapping, small_threshold, medium_threshold))\n",
    "\n",
    "size_data = []\n",
    "for defect_type in category_mapping.values():  # Iterate over all known defect types\n",
    "    for size_category in [\"Small\", \"Medium\", \"Large\"]:\n",
    "        size_data.append({\n",
    "            \"Defect Type\": defect_type,\n",
    "            \"Size Category\": size_category,\n",
    "            \"Original Count\": original_size_counts[defect_type].get(size_category, 0),\n",
    "            \"Sliced Count\": sliced_size_counts[defect_type].get(size_category, 0),\n",
    "            \"Deduplicated Sliced Count\": deduped_size_counts[defect_type].get(size_category, 0)\n",
    "        })\n",
    "\n",
    "# Create and display DataFrame\n",
    "df = pd.DataFrame(size_data).sort_values(by=[\"Defect Type\", \"Size Category\"]).reset_index(drop=True)\n",
    "print(\"Class Imbalance by Size Category\\n\", df)\n",
    "\n",
    "# Optional: Save deduplicated annotations back to the original file\n",
    "with open(f\"../data/coco_json_files/{coco_file_name}_sliced_coco.json\", \"w\") as f:\n",
    "    json.dump(sliced_annotations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_median_bbox_size(annotations, category_mapping):\n",
    "    bbox_sizes = defaultdict(list)\n",
    "    for ann in annotations['annotations']:\n",
    "        defect_type = category_mapping[ann['category_id']]\n",
    "        bbox_area = calculate_area(ann['bbox'])\n",
    "        bbox_sizes[defect_type].append(bbox_area)\n",
    "    return {defect_type: np.median(sizes) if sizes else 0 for defect_type, sizes in bbox_sizes.items()}\n",
    "\n",
    "\n",
    "def classify_defect_scale(median_bbox_sizes, category_mapping):\n",
    "    # Set a dynamic threshold using the median of non-zero median sizes\n",
    "    size_threshold = np.median([size for size in median_bbox_sizes.values() if size > 0])\n",
    "    return {\n",
    "        defect_type: \"Large Scale\" if median_size > size_threshold else \"Small Scale\"\n",
    "        for defect_type, median_size in median_bbox_sizes.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defect Type Scale Classification (Original)\n",
      "          Defect Type  Median Bounding Box Size Scale Classification\n",
      "0      blocked_valve              14686.476591          Large Scale\n",
      "1            unknown               2102.931893          Large Scale\n",
      "2         chip_crack               1112.886786          Large Scale\n",
      "3         line_crack                937.670135          Large Scale\n",
      "4        light_stain                878.451083          Small Scale\n",
      "5   improper_welding                864.219932          Small Scale\n",
      "6             bubble                160.517035          Small Scale\n",
      "7  particle_material                 83.051081          Small Scale\n",
      "\n",
      "Defect Type Scale Classification (Sliced)\n",
      "          Defect Type  Median Bounding Box Size Scale Classification\n",
      "0      blocked_valve              11270.149334          Large Scale\n",
      "1            unknown               1898.650919          Large Scale\n",
      "2         chip_crack               1113.572241          Large Scale\n",
      "3         line_crack                891.960240          Large Scale\n",
      "4        light_stain                846.513443          Small Scale\n",
      "5   improper_welding                759.592342          Small Scale\n",
      "6             bubble                159.710163          Small Scale\n",
      "7  particle_material                 80.816216          Small Scale\n"
     ]
    }
   ],
   "source": [
    "bbox_sizes = defaultdict(list)\n",
    "\n",
    "for ann in original_annotations['annotations']:\n",
    "    defect_type = category_mapping[ann['category_id']]\n",
    "    bbox_area = calculate_area(ann['bbox'])\n",
    "    bbox_sizes[defect_type].append(bbox_area)\n",
    "\n",
    "# Calculate the median size for each defect type, including all categories in the mapping\n",
    "median_bbox_sizes = {defect_type: np.median(sizes) if sizes else 0 for defect_type, sizes in bbox_sizes.items()}\n",
    "median_bbox_sizes = {defect_type: median_bbox_sizes.get(defect_type, 0) for defect_type in category_mapping.values()}\n",
    "\n",
    "# Determine a threshold for \"Small Scale\" vs \"Large Scale\" using the median of non-zero medians\n",
    "size_threshold = np.median([size for size in median_bbox_sizes.values() if size > 0])\n",
    "\n",
    "# Classify each defect type based on the threshold\n",
    "scale_classification = {\n",
    "    defect_type: \"Large Scale\" if median_size > size_threshold else \"Small Scale\"\n",
    "    for defect_type, median_size in median_bbox_sizes.items()\n",
    "}\n",
    "\n",
    "# Remove duplicates from sliced dataset\n",
    "unique_annotations, removed_ids = remove_duplicate_bboxes(sliced_annotations['annotations'], iou_threshold=0.7)\n",
    "sliced_annotations['annotations'] = unique_annotations  # Overwrite with deduplicated annotations\n",
    "\n",
    "# Calculate median bounding box sizes and classify scale for original annotations\n",
    "median_bbox_sizes_original = calculate_median_bbox_size(original_annotations, category_mapping)\n",
    "scale_classification_original = classify_defect_scale(median_bbox_sizes_original, category_mapping)\n",
    "\n",
    "# Calculate median bounding box sizes and classify scale for sliced annotations\n",
    "median_bbox_sizes_sliced = calculate_median_bbox_size(sliced_annotations, category_mapping)\n",
    "scale_classification_sliced = classify_defect_scale(median_bbox_sizes_sliced, category_mapping)\n",
    "\n",
    "# Prepare data for display\n",
    "scale_data_original = [\n",
    "    {\"Defect Type\": defect_type, \"Median Bounding Box Size\": median_size, \"Scale Classification\": scale_classification_original[defect_type]}\n",
    "    for defect_type, median_size in median_bbox_sizes_original.items()\n",
    "]\n",
    "\n",
    "scale_data_sliced = [\n",
    "    {\"Defect Type\": defect_type, \"Median Bounding Box Size\": median_size, \"Scale Classification\": scale_classification_sliced[defect_type]}\n",
    "    for defect_type, median_size in median_bbox_sizes_sliced.items()\n",
    "]\n",
    "\n",
    "# Create and display DataFrames\n",
    "df_original = pd.DataFrame(scale_data_original).sort_values(by=\"Median Bounding Box Size\", ascending=False).reset_index(drop=True)\n",
    "df_sliced = pd.DataFrame(scale_data_sliced).sort_values(by=\"Median Bounding Box Size\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nDefect Type Scale Classification (Original)\\n\", df_original)\n",
    "print(\"\\nDefect Type Scale Classification (Sliced)\\n\", df_sliced)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
