{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tA4GuUzCKmh",
        "outputId": "0ff990fc-e134-4a5b-9d19-8e2c9da5226b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class TiledSegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None, tile_size=(512, 512), overlap=51, grayscale=True, target_size=(3840, 2160)):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.tile_size = tile_size\n",
        "        self.overlap = overlap\n",
        "        self.grayscale = grayscale\n",
        "        self.target_size = target_size\n",
        "        self.images = [f.split('.')[0] for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
        "        self.tile_info = self._prepare_tile_info()  # Precompute all (image, tile_box) pairs\n",
        "\n",
        "    def _prepare_tile_info(self):\n",
        "        # Precompute tile boxes for each image to handle overlap and avoid recalculating in __getitem__\n",
        "        tile_info = []\n",
        "        tile_width, tile_height = self.tile_size\n",
        "\n",
        "        for img_name in self.images:\n",
        "            img_path = os.path.join(self.image_dir, f\"{img_name}.jpg\")\n",
        "            image = Image.open(img_path).resize(self.target_size, Image.BICUBIC)\n",
        "            img_width, img_height = image.size\n",
        "\n",
        "            for i in range(0, img_width - tile_width + 1, tile_width - self.overlap):\n",
        "                for j in range(0, img_height - tile_height + 1, tile_height - self.overlap):\n",
        "                    tile_info.append((img_name, (i, j, i + tile_width, j + tile_height)))\n",
        "        return tile_info\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tile_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, box = self.tile_info[idx]\n",
        "        img_path = os.path.join(self.image_dir, f\"{img_name}.jpg\")\n",
        "        mask_path = os.path.join(self.mask_dir, f\"{img_name}_lab.png\")\n",
        "\n",
        "        image = Image.open(img_path).resize(self.target_size, Image.BICUBIC)\n",
        "        mask = Image.open(mask_path).resize(self.target_size, Image.NEAREST).convert(\"L\")\n",
        "\n",
        "        if self.grayscale:\n",
        "            image = image.convert(\"L\")\n",
        "        else:\n",
        "            image = image.convert(\"RGB\")\n",
        "\n",
        "        # Crop image and mask\n",
        "        img_tile = image.crop(box)\n",
        "        mask_tile = mask.crop(box)\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            img_tile = self.transform(img_tile)\n",
        "        else:\n",
        "            img_tile = transforms.ToTensor()(img_tile)\n",
        "\n",
        "        mask_tile = torch.tensor(np.array(mask_tile), dtype=torch.long)\n",
        "\n",
        "        return img_tile, mask_tile  # Now returns individual (image tile, mask tile) pairs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EETVHD84VYT7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Paths to each subset\n",
        "target_dir = '/content/drive/MyDrive/RescueNet_dataset/RescueNet'\n",
        "\n",
        "train_image_dir = os.path.join(target_dir, 'train/train-org-img')\n",
        "train_mask_dir = os.path.join(target_dir, 'train/train-label-img')\n",
        "\n",
        "val_image_dir = os.path.join(target_dir, 'val/val-org-img')\n",
        "val_mask_dir = os.path.join(target_dir, 'val/val-label-img')\n",
        "\n",
        "test_image_dir = os.path.join(target_dir, 'test/test-org-img')\n",
        "test_mask_dir = os.path.join(target_dir, 'test/test-label-img')\n",
        "\n",
        "grayscale = False\n",
        "tile_size = (512, 512)\n",
        "batch_size = 8\n",
        "overlap = 51  # 10% overlap for 512 x 512 tiles\n",
        "target_size = (3840, 2160)  # Resize images to 3840 x 2160\n",
        "\n",
        "# Example transformations for image tiles\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]) if grayscale else transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Instantiate the dataset\n",
        "train_dataset = TiledSegmentationDataset(\n",
        "    image_dir=train_image_dir,\n",
        "    mask_dir=train_mask_dir,\n",
        "    transform=transform,\n",
        "    tile_size=tile_size,\n",
        "    overlap=overlap,\n",
        "    grayscale=grayscale,\n",
        "    target_size=target_size\n",
        ")\n",
        "\n",
        "val_dataset = TiledSegmentationDataset(\n",
        "    image_dir=val_image_dir,\n",
        "    mask_dir=val_mask_dir,\n",
        "    transform=transform,\n",
        "    tile_size=tile_size,\n",
        "    overlap=overlap,\n",
        "    grayscale=grayscale,\n",
        "    target_size=target_size\n",
        ")\n",
        "\n",
        "test_dataset = TiledSegmentationDataset(\n",
        "    image_dir=test_image_dir,\n",
        "    mask_dir=test_mask_dir,\n",
        "    transform=transform,\n",
        "    tile_size=tile_size,\n",
        "    overlap=overlap,\n",
        "    grayscale=grayscale,\n",
        "    target_size=target_size\n",
        ")\n",
        "\n",
        "# Wrap each dataset in a DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "yyYVknNklZcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "class JPU(nn.Module):\n",
        "    def __init__(self, in_channels, width=512):\n",
        "        super(JPU, self).__init__()\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels[0], width, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(width),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels[1], width, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(width),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels[2], width, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(width),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, *inputs):\n",
        "        feats = [self.conv5(inputs[0]), self.conv4(inputs[1]), self.conv3(inputs[2])]\n",
        "        size = feats[-1].shape[2:]\n",
        "        feats = [F.interpolate(f, size=size, mode='bilinear', align_corners=True) for f in feats]\n",
        "        feat = torch.cat(feats, dim=1)\n",
        "        return feat\n",
        "\n",
        "class FastFCN(nn.Module):\n",
        "    def __init__(self, num_classes=12):\n",
        "        super(FastFCN, self).__init__()\n",
        "\n",
        "        # Load a ResNet-50 backbone pretrained on ImageNet\n",
        "        backbone = models.resnet50(pretrained=True)\n",
        "\n",
        "        # Extract feature layers from ResNet\n",
        "        self.layer0 = nn.Sequential(\n",
        "            backbone.conv1, backbone.bn1, backbone.relu, backbone.maxpool)\n",
        "        self.layer1 = backbone.layer1\n",
        "        self.layer2 = backbone.layer2\n",
        "        self.layer3 = backbone.layer3\n",
        "        self.layer4 = backbone.layer4\n",
        "\n",
        "        # JPU module with correct input channels\n",
        "        self.jpu = JPU([2048, 1024, 512], width=512)  # Adjusted to match actual channels\n",
        "\n",
        "        # Segmentation Head\n",
        "        self.seg_head = nn.Sequential(\n",
        "            nn.Conv2d(3 * 512, 512, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, num_classes, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from backbone\n",
        "        x0 = self.layer0(x)\n",
        "        # print(\"Layer 0 output:\", x0.shape)\n",
        "        x1 = self.layer1(x0)\n",
        "        # print(\"Layer 1 output:\", x1.shape)\n",
        "        x2 = self.layer2(x1)\n",
        "        # print(\"Layer 2 output:\", x2.shape)\n",
        "        x3 = self.layer3(x2)\n",
        "        # print(\"Layer 3 output:\", x3.shape)\n",
        "        x4 = self.layer4(x3)\n",
        "        # print(\"Layer 4 output:\", x4.shape)\n",
        "\n",
        "        # Apply JPU\n",
        "        jpu_feat = self.jpu(x4, x3, x2)\n",
        "        # print(\"JPU output:\", jpu_feat.shape)\n",
        "\n",
        "        # Apply segmentation head\n",
        "        out = self.seg_head(jpu_feat)\n",
        "        # print(\"Segmentation head output:\", out.shape)\n",
        "\n",
        "        # Upsample to the input image size\n",
        "        out = F.interpolate(out, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
        "        return out\n",
        "\n",
        "# Instantiate model with 12 classes\n",
        "model = FastFCN(num_classes=12)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "id": "xxUmCe-nszLM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, num_epochs, device):\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        print(f\"\\nEpoch [{epoch + 1}/{num_epochs}]\")\n",
        "\n",
        "        for batch_idx, (images, masks) in enumerate(dataloader):\n",
        "            batch_start_time = time.time()\n",
        "\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update running loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Calculate time per batch\n",
        "            batch_time = time.time() - batch_start_time\n",
        "            print(f\"Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}, Time: {batch_time:.2f} seconds\")\n",
        "\n",
        "        # Calculate epoch loss and time\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        avg_batch_time = epoch_time / len(dataloader)\n",
        "\n",
        "        print(f\"Epoch Loss: {epoch_loss:.4f}\")\n",
        "        print(f\"Epoch Time: {epoch_time:.2f} seconds, Avg Time per Batch: {avg_batch_time:.2f} seconds\")\n",
        "\n",
        "        # Estimate remaining time based on current epoch time\n",
        "        remaining_time = epoch_time * (num_epochs - epoch - 1)\n",
        "        print(f\"Estimated Time Remaining: {remaining_time // 60:.0f}m {remaining_time % 60:.0f}s\")\n",
        "\n",
        "num_epochs = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using:\", device)\n",
        "\n",
        "train(model, train_loader, criterion, optimizer, num_epochs=num_epochs, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XRsoJe7Ks46D",
        "outputId": "7563fe38-f68c-4492-e974-cc9c853dce02"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda\n",
            "\n",
            "Epoch [1/2]\n",
            "Batch 1/940, Loss: 2.7324, Time: 0.72 seconds\n",
            "Batch 2/940, Loss: 2.8195, Time: 0.74 seconds\n",
            "Batch 3/940, Loss: 1.7334, Time: 0.73 seconds\n",
            "Batch 4/940, Loss: 1.2127, Time: 0.75 seconds\n",
            "Batch 5/940, Loss: 1.3647, Time: 0.74 seconds\n",
            "Batch 6/940, Loss: 1.2291, Time: 0.74 seconds\n",
            "Batch 7/940, Loss: 2.9021, Time: 0.76 seconds\n",
            "Batch 8/940, Loss: 2.1381, Time: 0.73 seconds\n",
            "Batch 9/940, Loss: 1.2285, Time: 0.73 seconds\n",
            "Batch 10/940, Loss: 2.0755, Time: 0.72 seconds\n",
            "Batch 11/940, Loss: 1.4923, Time: 0.74 seconds\n",
            "Batch 12/940, Loss: 1.4276, Time: 0.71 seconds\n",
            "Batch 13/940, Loss: 0.8928, Time: 0.71 seconds\n",
            "Batch 14/940, Loss: 0.6425, Time: 0.74 seconds\n",
            "Batch 15/940, Loss: 2.1208, Time: 0.71 seconds\n",
            "Batch 16/940, Loss: 2.0969, Time: 0.75 seconds\n",
            "Batch 17/940, Loss: 1.2724, Time: 0.72 seconds\n",
            "Batch 18/940, Loss: 1.0774, Time: 0.75 seconds\n",
            "Batch 19/940, Loss: 0.8766, Time: 0.74 seconds\n",
            "Batch 20/940, Loss: 0.8442, Time: 0.74 seconds\n",
            "Batch 21/940, Loss: 1.2474, Time: 0.71 seconds\n",
            "Batch 22/940, Loss: 1.2144, Time: 0.74 seconds\n",
            "Batch 23/940, Loss: 0.9615, Time: 0.74 seconds\n",
            "Batch 24/940, Loss: 1.0620, Time: 0.72 seconds\n",
            "Batch 25/940, Loss: 0.9508, Time: 0.74 seconds\n",
            "Batch 26/940, Loss: 0.9050, Time: 0.75 seconds\n",
            "Batch 27/940, Loss: 1.0810, Time: 0.71 seconds\n",
            "Batch 28/940, Loss: 0.9950, Time: 0.71 seconds\n",
            "Batch 29/940, Loss: 0.8882, Time: 0.74 seconds\n",
            "Batch 30/940, Loss: 1.0931, Time: 0.74 seconds\n",
            "Batch 31/940, Loss: 0.8620, Time: 0.72 seconds\n",
            "Batch 32/940, Loss: 1.2427, Time: 0.71 seconds\n",
            "Batch 33/940, Loss: 0.8190, Time: 0.75 seconds\n",
            "Batch 34/940, Loss: 1.4286, Time: 0.72 seconds\n",
            "Batch 35/940, Loss: 1.7670, Time: 0.72 seconds\n",
            "Batch 36/940, Loss: 0.5807, Time: 0.72 seconds\n",
            "Batch 37/940, Loss: 0.8064, Time: 0.75 seconds\n",
            "Batch 38/940, Loss: 0.5911, Time: 0.74 seconds\n",
            "Batch 39/940, Loss: 0.9215, Time: 0.72 seconds\n",
            "Batch 40/940, Loss: 0.6922, Time: 0.72 seconds\n",
            "Batch 41/940, Loss: 1.3271, Time: 0.75 seconds\n",
            "Batch 42/940, Loss: 1.0372, Time: 0.71 seconds\n",
            "Batch 43/940, Loss: 2.2491, Time: 0.71 seconds\n",
            "Batch 44/940, Loss: 1.1311, Time: 0.72 seconds\n",
            "Batch 45/940, Loss: 1.7443, Time: 0.72 seconds\n",
            "Batch 46/940, Loss: 0.8446, Time: 0.71 seconds\n",
            "Batch 47/940, Loss: 0.8942, Time: 0.74 seconds\n",
            "Batch 48/940, Loss: 0.9721, Time: 0.75 seconds\n",
            "Batch 49/940, Loss: 0.8105, Time: 0.75 seconds\n",
            "Batch 50/940, Loss: 1.9802, Time: 0.75 seconds\n",
            "Batch 51/940, Loss: 1.0281, Time: 0.74 seconds\n",
            "Batch 52/940, Loss: 0.7735, Time: 0.71 seconds\n",
            "Batch 53/940, Loss: 0.9276, Time: 0.75 seconds\n",
            "Batch 54/940, Loss: 1.2927, Time: 0.74 seconds\n",
            "Batch 55/940, Loss: 0.5305, Time: 0.72 seconds\n",
            "Batch 56/940, Loss: 1.2371, Time: 0.73 seconds\n",
            "Batch 57/940, Loss: 1.2284, Time: 0.72 seconds\n",
            "Batch 58/940, Loss: 1.5828, Time: 0.74 seconds\n",
            "Batch 59/940, Loss: 0.9285, Time: 0.75 seconds\n",
            "Batch 60/940, Loss: 0.7429, Time: 0.75 seconds\n",
            "Batch 61/940, Loss: 1.0762, Time: 0.72 seconds\n",
            "Batch 62/940, Loss: 0.8217, Time: 0.71 seconds\n",
            "Batch 63/940, Loss: 0.7494, Time: 0.72 seconds\n",
            "Batch 64/940, Loss: 0.8953, Time: 0.74 seconds\n",
            "Batch 65/940, Loss: 0.5336, Time: 0.74 seconds\n",
            "Batch 66/940, Loss: 0.7681, Time: 0.72 seconds\n",
            "Batch 67/940, Loss: 1.3233, Time: 0.75 seconds\n",
            "Batch 68/940, Loss: 0.8478, Time: 0.74 seconds\n",
            "Batch 69/940, Loss: 0.5072, Time: 0.75 seconds\n",
            "Batch 70/940, Loss: 1.2153, Time: 0.73 seconds\n",
            "Batch 71/940, Loss: 0.9359, Time: 0.72 seconds\n",
            "Batch 72/940, Loss: 1.1003, Time: 0.71 seconds\n",
            "Batch 73/940, Loss: 0.9912, Time: 0.71 seconds\n",
            "Batch 74/940, Loss: 1.1250, Time: 0.71 seconds\n",
            "Batch 75/940, Loss: 0.5455, Time: 0.71 seconds\n",
            "Batch 76/940, Loss: 0.7597, Time: 0.71 seconds\n",
            "Batch 77/940, Loss: 1.4290, Time: 0.72 seconds\n",
            "Batch 78/940, Loss: 1.2452, Time: 0.75 seconds\n",
            "Batch 79/940, Loss: 0.8408, Time: 0.71 seconds\n",
            "Batch 80/940, Loss: 0.7745, Time: 0.71 seconds\n",
            "Batch 81/940, Loss: 0.6466, Time: 0.74 seconds\n",
            "Batch 82/940, Loss: 0.9443, Time: 0.73 seconds\n",
            "Batch 83/940, Loss: 0.6410, Time: 0.73 seconds\n",
            "Batch 84/940, Loss: 1.3301, Time: 0.73 seconds\n",
            "Batch 85/940, Loss: 0.7850, Time: 0.75 seconds\n",
            "Batch 86/940, Loss: 0.4293, Time: 0.73 seconds\n",
            "Batch 87/940, Loss: 1.2141, Time: 0.73 seconds\n",
            "Batch 88/940, Loss: 1.9526, Time: 0.74 seconds\n",
            "Batch 89/940, Loss: 0.4894, Time: 0.73 seconds\n",
            "Batch 90/940, Loss: 0.6988, Time: 0.73 seconds\n",
            "Batch 91/940, Loss: 1.1426, Time: 0.72 seconds\n",
            "Batch 92/940, Loss: 0.5419, Time: 0.71 seconds\n",
            "Batch 93/940, Loss: 1.0012, Time: 0.74 seconds\n",
            "Batch 94/940, Loss: 0.6243, Time: 0.73 seconds\n",
            "Batch 95/940, Loss: 0.6642, Time: 0.74 seconds\n",
            "Batch 96/940, Loss: 1.0112, Time: 0.72 seconds\n",
            "Batch 97/940, Loss: 1.0978, Time: 0.72 seconds\n",
            "Batch 98/940, Loss: 1.6178, Time: 0.75 seconds\n",
            "Batch 99/940, Loss: 0.6971, Time: 0.75 seconds\n",
            "Batch 100/940, Loss: 1.2787, Time: 0.71 seconds\n",
            "Batch 101/940, Loss: 0.8488, Time: 0.71 seconds\n",
            "Batch 102/940, Loss: 0.3613, Time: 0.71 seconds\n",
            "Batch 103/940, Loss: 1.2451, Time: 0.73 seconds\n",
            "Batch 104/940, Loss: 1.2685, Time: 0.74 seconds\n",
            "Batch 105/940, Loss: 0.6569, Time: 0.72 seconds\n",
            "Batch 106/940, Loss: 0.8674, Time: 0.74 seconds\n",
            "Batch 107/940, Loss: 0.9152, Time: 0.74 seconds\n",
            "Batch 108/940, Loss: 0.7117, Time: 0.72 seconds\n",
            "Batch 109/940, Loss: 0.4618, Time: 0.73 seconds\n",
            "Batch 110/940, Loss: 0.8154, Time: 0.74 seconds\n",
            "Batch 111/940, Loss: 1.0477, Time: 0.71 seconds\n",
            "Batch 112/940, Loss: 0.9816, Time: 0.72 seconds\n",
            "Batch 113/940, Loss: 0.7098, Time: 0.72 seconds\n",
            "Batch 114/940, Loss: 0.6886, Time: 0.71 seconds\n",
            "Batch 115/940, Loss: 1.2690, Time: 0.74 seconds\n",
            "Batch 116/940, Loss: 1.1053, Time: 0.72 seconds\n",
            "Batch 117/940, Loss: 0.7360, Time: 0.72 seconds\n",
            "Batch 118/940, Loss: 0.7466, Time: 0.74 seconds\n",
            "Batch 119/940, Loss: 0.7146, Time: 0.74 seconds\n",
            "Batch 120/940, Loss: 0.9120, Time: 0.72 seconds\n",
            "Batch 121/940, Loss: 0.5822, Time: 0.72 seconds\n",
            "Batch 122/940, Loss: 0.7346, Time: 0.73 seconds\n",
            "Batch 123/940, Loss: 0.7576, Time: 0.73 seconds\n",
            "Batch 124/940, Loss: 0.9195, Time: 0.71 seconds\n",
            "Batch 125/940, Loss: 0.9497, Time: 0.71 seconds\n",
            "Batch 126/940, Loss: 0.6077, Time: 0.71 seconds\n",
            "Batch 127/940, Loss: 0.4527, Time: 0.73 seconds\n",
            "Batch 128/940, Loss: 0.7795, Time: 0.71 seconds\n",
            "Batch 129/940, Loss: 0.5853, Time: 0.72 seconds\n",
            "Batch 130/940, Loss: 1.1262, Time: 0.73 seconds\n",
            "Batch 131/940, Loss: 0.8293, Time: 0.74 seconds\n",
            "Batch 132/940, Loss: 0.9116, Time: 0.75 seconds\n",
            "Batch 133/940, Loss: 0.5729, Time: 0.75 seconds\n",
            "Batch 134/940, Loss: 0.9823, Time: 0.74 seconds\n",
            "Batch 135/940, Loss: 1.0575, Time: 0.74 seconds\n",
            "Batch 136/940, Loss: 1.2135, Time: 0.73 seconds\n",
            "Batch 137/940, Loss: 0.5974, Time: 0.71 seconds\n",
            "Batch 138/940, Loss: 1.0078, Time: 0.71 seconds\n",
            "Batch 139/940, Loss: 0.3144, Time: 0.74 seconds\n",
            "Batch 140/940, Loss: 0.5361, Time: 0.71 seconds\n",
            "Batch 141/940, Loss: 1.2495, Time: 0.72 seconds\n",
            "Batch 142/940, Loss: 0.8188, Time: 0.74 seconds\n",
            "Batch 143/940, Loss: 0.6848, Time: 0.74 seconds\n",
            "Batch 144/940, Loss: 1.0911, Time: 0.75 seconds\n",
            "Batch 145/940, Loss: 0.3324, Time: 0.73 seconds\n",
            "Batch 146/940, Loss: 0.4951, Time: 0.73 seconds\n",
            "Batch 147/940, Loss: 0.5436, Time: 0.73 seconds\n",
            "Batch 148/940, Loss: 0.6386, Time: 0.74 seconds\n",
            "Batch 149/940, Loss: 0.5999, Time: 0.73 seconds\n",
            "Batch 150/940, Loss: 0.6655, Time: 0.73 seconds\n",
            "Batch 151/940, Loss: 0.3866, Time: 0.71 seconds\n",
            "Batch 152/940, Loss: 0.9613, Time: 0.74 seconds\n",
            "Batch 153/940, Loss: 0.6205, Time: 0.73 seconds\n",
            "Batch 154/940, Loss: 0.7631, Time: 0.71 seconds\n",
            "Batch 155/940, Loss: 0.9991, Time: 0.71 seconds\n",
            "Batch 156/940, Loss: 0.4277, Time: 0.74 seconds\n",
            "Batch 157/940, Loss: 0.4694, Time: 0.73 seconds\n",
            "Batch 158/940, Loss: 0.5657, Time: 0.73 seconds\n",
            "Batch 159/940, Loss: 0.5732, Time: 0.73 seconds\n",
            "Batch 160/940, Loss: 1.0628, Time: 0.72 seconds\n",
            "Batch 161/940, Loss: 0.6018, Time: 0.71 seconds\n",
            "Batch 162/940, Loss: 0.9840, Time: 0.71 seconds\n",
            "Batch 163/940, Loss: 0.7270, Time: 0.75 seconds\n",
            "Batch 164/940, Loss: 0.3179, Time: 0.75 seconds\n",
            "Batch 165/940, Loss: 0.9681, Time: 0.74 seconds\n",
            "Batch 166/940, Loss: 0.3176, Time: 0.72 seconds\n",
            "Batch 167/940, Loss: 0.7793, Time: 0.73 seconds\n",
            "Batch 168/940, Loss: 0.6126, Time: 0.74 seconds\n",
            "Batch 169/940, Loss: 0.7401, Time: 0.71 seconds\n",
            "Batch 170/940, Loss: 0.7254, Time: 0.71 seconds\n",
            "Batch 171/940, Loss: 0.6514, Time: 0.73 seconds\n",
            "Batch 172/940, Loss: 0.4018, Time: 0.74 seconds\n",
            "Batch 173/940, Loss: 0.8986, Time: 0.74 seconds\n",
            "Batch 174/940, Loss: 0.5740, Time: 0.75 seconds\n",
            "Batch 175/940, Loss: 0.4141, Time: 0.72 seconds\n",
            "Batch 176/940, Loss: 1.2440, Time: 0.74 seconds\n",
            "Batch 177/940, Loss: 0.8895, Time: 0.75 seconds\n",
            "Batch 178/940, Loss: 1.0091, Time: 0.73 seconds\n",
            "Batch 179/940, Loss: 0.5104, Time: 0.74 seconds\n",
            "Batch 180/940, Loss: 0.4174, Time: 0.74 seconds\n",
            "Batch 181/940, Loss: 1.1004, Time: 0.74 seconds\n",
            "Batch 182/940, Loss: 0.9887, Time: 0.75 seconds\n",
            "Batch 183/940, Loss: 0.6520, Time: 0.75 seconds\n",
            "Batch 184/940, Loss: 0.4571, Time: 0.71 seconds\n",
            "Batch 185/940, Loss: 1.0356, Time: 0.71 seconds\n",
            "Batch 186/940, Loss: 2.0812, Time: 0.72 seconds\n",
            "Batch 187/940, Loss: 0.3572, Time: 0.74 seconds\n",
            "Batch 188/940, Loss: 1.2542, Time: 0.73 seconds\n",
            "Batch 189/940, Loss: 0.5178, Time: 0.75 seconds\n",
            "Batch 190/940, Loss: 0.6364, Time: 0.72 seconds\n",
            "Batch 191/940, Loss: 0.6719, Time: 0.72 seconds\n",
            "Batch 192/940, Loss: 0.5922, Time: 0.73 seconds\n",
            "Batch 193/940, Loss: 0.7954, Time: 0.71 seconds\n",
            "Batch 194/940, Loss: 1.3531, Time: 0.71 seconds\n",
            "Batch 195/940, Loss: 0.4495, Time: 0.72 seconds\n",
            "Batch 196/940, Loss: 0.7472, Time: 0.72 seconds\n",
            "Batch 197/940, Loss: 0.4118, Time: 0.73 seconds\n",
            "Batch 198/940, Loss: 0.7741, Time: 0.73 seconds\n",
            "Batch 199/940, Loss: 0.5962, Time: 0.73 seconds\n",
            "Batch 200/940, Loss: 0.5150, Time: 0.74 seconds\n",
            "Batch 201/940, Loss: 0.7662, Time: 0.73 seconds\n",
            "Batch 202/940, Loss: 0.6457, Time: 0.73 seconds\n",
            "Batch 203/940, Loss: 1.7653, Time: 0.74 seconds\n",
            "Batch 204/940, Loss: 0.6605, Time: 0.75 seconds\n",
            "Batch 205/940, Loss: 0.4997, Time: 0.74 seconds\n",
            "Batch 206/940, Loss: 0.7100, Time: 0.73 seconds\n",
            "Batch 207/940, Loss: 0.4820, Time: 0.72 seconds\n",
            "Batch 208/940, Loss: 0.4680, Time: 0.73 seconds\n",
            "Batch 209/940, Loss: 0.6799, Time: 0.71 seconds\n",
            "Batch 210/940, Loss: 1.0284, Time: 0.72 seconds\n",
            "Batch 211/940, Loss: 0.9733, Time: 0.76 seconds\n",
            "Batch 212/940, Loss: 0.6814, Time: 0.74 seconds\n",
            "Batch 213/940, Loss: 0.4129, Time: 0.75 seconds\n",
            "Batch 214/940, Loss: 0.8558, Time: 0.75 seconds\n",
            "Batch 215/940, Loss: 0.5856, Time: 0.73 seconds\n",
            "Batch 216/940, Loss: 0.5573, Time: 0.72 seconds\n",
            "Batch 217/940, Loss: 0.8564, Time: 0.75 seconds\n",
            "Batch 218/940, Loss: 0.3403, Time: 0.74 seconds\n",
            "Batch 219/940, Loss: 0.8181, Time: 0.74 seconds\n",
            "Batch 220/940, Loss: 0.7873, Time: 0.71 seconds\n",
            "Batch 221/940, Loss: 0.5814, Time: 0.71 seconds\n",
            "Batch 222/940, Loss: 0.5698, Time: 0.71 seconds\n",
            "Batch 223/940, Loss: 0.4583, Time: 0.71 seconds\n",
            "Batch 224/940, Loss: 0.4074, Time: 0.74 seconds\n",
            "Batch 225/940, Loss: 0.5605, Time: 0.73 seconds\n",
            "Batch 226/940, Loss: 0.4868, Time: 0.71 seconds\n",
            "Batch 227/940, Loss: 0.9641, Time: 0.72 seconds\n",
            "Batch 228/940, Loss: 0.5959, Time: 0.73 seconds\n",
            "Batch 229/940, Loss: 0.4146, Time: 0.71 seconds\n",
            "Batch 230/940, Loss: 0.3606, Time: 0.71 seconds\n",
            "Batch 231/940, Loss: 0.7638, Time: 0.75 seconds\n",
            "Batch 232/940, Loss: 0.7146, Time: 0.75 seconds\n",
            "Batch 233/940, Loss: 0.5709, Time: 0.74 seconds\n",
            "Batch 234/940, Loss: 0.5105, Time: 0.73 seconds\n",
            "Batch 235/940, Loss: 1.4024, Time: 0.73 seconds\n",
            "Batch 236/940, Loss: 0.7510, Time: 0.72 seconds\n",
            "Batch 237/940, Loss: 0.6432, Time: 0.71 seconds\n",
            "Batch 238/940, Loss: 0.9288, Time: 0.71 seconds\n",
            "Batch 239/940, Loss: 0.4534, Time: 0.72 seconds\n",
            "Batch 240/940, Loss: 0.5878, Time: 0.72 seconds\n",
            "Batch 241/940, Loss: 0.7460, Time: 0.72 seconds\n",
            "Batch 242/940, Loss: 1.4931, Time: 0.73 seconds\n",
            "Batch 243/940, Loss: 0.4098, Time: 0.72 seconds\n",
            "Batch 244/940, Loss: 0.5250, Time: 0.75 seconds\n",
            "Batch 245/940, Loss: 0.5856, Time: 0.75 seconds\n",
            "Batch 246/940, Loss: 0.6102, Time: 0.74 seconds\n",
            "Batch 247/940, Loss: 0.9220, Time: 0.73 seconds\n",
            "Batch 248/940, Loss: 0.3475, Time: 0.74 seconds\n",
            "Batch 249/940, Loss: 0.9941, Time: 0.73 seconds\n",
            "Batch 250/940, Loss: 0.6220, Time: 0.72 seconds\n",
            "Batch 251/940, Loss: 2.0199, Time: 0.74 seconds\n",
            "Batch 252/940, Loss: 0.6620, Time: 0.74 seconds\n",
            "Batch 253/940, Loss: 0.9392, Time: 0.75 seconds\n",
            "Batch 254/940, Loss: 0.5361, Time: 0.72 seconds\n",
            "Batch 255/940, Loss: 0.3139, Time: 0.71 seconds\n",
            "Batch 256/940, Loss: 0.6982, Time: 0.71 seconds\n",
            "Batch 257/940, Loss: 0.6620, Time: 0.72 seconds\n",
            "Batch 258/940, Loss: 0.7275, Time: 0.72 seconds\n",
            "Batch 259/940, Loss: 0.6872, Time: 0.74 seconds\n",
            "Batch 260/940, Loss: 0.7604, Time: 0.74 seconds\n",
            "Batch 261/940, Loss: 0.5103, Time: 0.73 seconds\n",
            "Batch 262/940, Loss: 0.6190, Time: 0.73 seconds\n",
            "Batch 263/940, Loss: 0.6529, Time: 0.73 seconds\n",
            "Batch 264/940, Loss: 0.6788, Time: 0.72 seconds\n",
            "Batch 265/940, Loss: 0.7105, Time: 0.71 seconds\n",
            "Batch 266/940, Loss: 0.4503, Time: 0.75 seconds\n",
            "Batch 267/940, Loss: 0.5173, Time: 0.74 seconds\n",
            "Batch 268/940, Loss: 0.9854, Time: 0.73 seconds\n",
            "Batch 269/940, Loss: 0.9839, Time: 0.72 seconds\n",
            "Batch 270/940, Loss: 1.3064, Time: 0.72 seconds\n",
            "Batch 271/940, Loss: 0.5012, Time: 0.74 seconds\n",
            "Batch 272/940, Loss: 0.8569, Time: 0.73 seconds\n",
            "Batch 273/940, Loss: 0.8697, Time: 0.71 seconds\n",
            "Batch 274/940, Loss: 1.0133, Time: 0.73 seconds\n",
            "Batch 275/940, Loss: 0.5595, Time: 0.72 seconds\n",
            "Batch 276/940, Loss: 1.4154, Time: 0.71 seconds\n",
            "Batch 277/940, Loss: 1.1522, Time: 0.72 seconds\n",
            "Batch 278/940, Loss: 1.0352, Time: 0.73 seconds\n",
            "Batch 279/940, Loss: 0.7399, Time: 0.72 seconds\n",
            "Batch 280/940, Loss: 0.5294, Time: 0.71 seconds\n",
            "Batch 281/940, Loss: 0.3421, Time: 0.72 seconds\n",
            "Batch 282/940, Loss: 0.5784, Time: 0.72 seconds\n",
            "Batch 283/940, Loss: 0.9196, Time: 0.73 seconds\n",
            "Batch 284/940, Loss: 0.7657, Time: 0.72 seconds\n",
            "Batch 285/940, Loss: 1.1169, Time: 0.74 seconds\n",
            "Batch 286/940, Loss: 0.8093, Time: 0.72 seconds\n",
            "Batch 287/940, Loss: 0.6676, Time: 0.72 seconds\n",
            "Batch 288/940, Loss: 0.7464, Time: 0.74 seconds\n",
            "Batch 289/940, Loss: 0.6758, Time: 0.74 seconds\n",
            "Batch 290/940, Loss: 0.6561, Time: 0.74 seconds\n",
            "Batch 291/940, Loss: 0.8352, Time: 0.71 seconds\n",
            "Batch 292/940, Loss: 0.6933, Time: 0.71 seconds\n",
            "Batch 293/940, Loss: 0.4713, Time: 0.73 seconds\n",
            "Batch 294/940, Loss: 0.6982, Time: 0.72 seconds\n",
            "Batch 295/940, Loss: 1.0571, Time: 0.71 seconds\n",
            "Batch 296/940, Loss: 0.6128, Time: 0.73 seconds\n",
            "Batch 297/940, Loss: 0.3763, Time: 0.72 seconds\n",
            "Batch 298/940, Loss: 0.6151, Time: 0.73 seconds\n",
            "Batch 299/940, Loss: 0.2900, Time: 0.74 seconds\n",
            "Batch 300/940, Loss: 0.4905, Time: 0.73 seconds\n",
            "Batch 301/940, Loss: 0.4140, Time: 0.71 seconds\n",
            "Batch 302/940, Loss: 1.0742, Time: 0.71 seconds\n",
            "Batch 303/940, Loss: 0.6300, Time: 0.71 seconds\n",
            "Batch 304/940, Loss: 0.7798, Time: 0.74 seconds\n",
            "Batch 305/940, Loss: 0.8234, Time: 0.74 seconds\n",
            "Batch 306/940, Loss: 0.5402, Time: 0.75 seconds\n",
            "Batch 307/940, Loss: 2.3704, Time: 0.71 seconds\n",
            "Batch 308/940, Loss: 0.4940, Time: 0.72 seconds\n",
            "Batch 309/940, Loss: 0.9226, Time: 0.75 seconds\n",
            "Batch 310/940, Loss: 0.5068, Time: 0.73 seconds\n",
            "Batch 311/940, Loss: 0.8097, Time: 0.72 seconds\n",
            "Batch 312/940, Loss: 0.6554, Time: 0.75 seconds\n",
            "Batch 313/940, Loss: 1.1881, Time: 0.75 seconds\n",
            "Batch 314/940, Loss: 1.0255, Time: 0.72 seconds\n",
            "Batch 315/940, Loss: 0.5742, Time: 0.73 seconds\n",
            "Batch 316/940, Loss: 0.6364, Time: 0.71 seconds\n",
            "Batch 317/940, Loss: 0.5971, Time: 0.72 seconds\n",
            "Batch 318/940, Loss: 0.9510, Time: 0.72 seconds\n",
            "Batch 319/940, Loss: 1.1416, Time: 0.71 seconds\n",
            "Batch 320/940, Loss: 0.8594, Time: 0.71 seconds\n",
            "Batch 321/940, Loss: 0.8435, Time: 0.73 seconds\n",
            "Batch 322/940, Loss: 0.6299, Time: 0.74 seconds\n",
            "Batch 323/940, Loss: 0.3626, Time: 0.74 seconds\n",
            "Batch 324/940, Loss: 0.5594, Time: 0.73 seconds\n",
            "Batch 325/940, Loss: 0.5367, Time: 0.73 seconds\n",
            "Batch 326/940, Loss: 1.1880, Time: 0.71 seconds\n",
            "Batch 327/940, Loss: 0.5097, Time: 0.74 seconds\n",
            "Batch 328/940, Loss: 0.4622, Time: 0.72 seconds\n",
            "Batch 329/940, Loss: 1.1819, Time: 0.72 seconds\n",
            "Batch 330/940, Loss: 0.4231, Time: 0.71 seconds\n",
            "Batch 331/940, Loss: 0.7458, Time: 0.74 seconds\n",
            "Batch 332/940, Loss: 0.6587, Time: 0.75 seconds\n",
            "Batch 333/940, Loss: 0.7126, Time: 0.74 seconds\n",
            "Batch 334/940, Loss: 0.8370, Time: 0.73 seconds\n",
            "Batch 335/940, Loss: 0.5864, Time: 0.74 seconds\n",
            "Batch 336/940, Loss: 0.6910, Time: 0.74 seconds\n",
            "Batch 337/940, Loss: 0.2950, Time: 0.71 seconds\n",
            "Batch 338/940, Loss: 0.8189, Time: 0.73 seconds\n",
            "Batch 339/940, Loss: 0.4552, Time: 0.74 seconds\n",
            "Batch 340/940, Loss: 1.0416, Time: 0.74 seconds\n",
            "Batch 341/940, Loss: 1.3532, Time: 0.74 seconds\n",
            "Batch 342/940, Loss: 1.3216, Time: 0.73 seconds\n",
            "Batch 343/940, Loss: 1.1120, Time: 0.71 seconds\n",
            "Batch 344/940, Loss: 0.5916, Time: 0.71 seconds\n",
            "Batch 345/940, Loss: 0.6301, Time: 0.72 seconds\n",
            "Batch 346/940, Loss: 0.9497, Time: 0.72 seconds\n",
            "Batch 347/940, Loss: 0.6156, Time: 0.74 seconds\n",
            "Batch 348/940, Loss: 0.3702, Time: 0.75 seconds\n",
            "Batch 349/940, Loss: 0.6885, Time: 0.71 seconds\n",
            "Batch 350/940, Loss: 0.3921, Time: 0.72 seconds\n",
            "Batch 351/940, Loss: 0.3915, Time: 0.71 seconds\n",
            "Batch 352/940, Loss: 0.5034, Time: 0.74 seconds\n",
            "Batch 353/940, Loss: 1.4663, Time: 0.73 seconds\n",
            "Batch 354/940, Loss: 0.4385, Time: 0.75 seconds\n",
            "Batch 355/940, Loss: 0.9250, Time: 0.72 seconds\n",
            "Batch 356/940, Loss: 0.6176, Time: 0.72 seconds\n",
            "Batch 357/940, Loss: 0.4520, Time: 0.73 seconds\n",
            "Batch 358/940, Loss: 0.6007, Time: 0.74 seconds\n",
            "Batch 359/940, Loss: 0.6924, Time: 0.75 seconds\n",
            "Batch 360/940, Loss: 1.0220, Time: 0.74 seconds\n",
            "Batch 361/940, Loss: 0.6449, Time: 0.73 seconds\n",
            "Batch 362/940, Loss: 0.5024, Time: 0.71 seconds\n",
            "Batch 363/940, Loss: 0.7000, Time: 0.71 seconds\n",
            "Batch 364/940, Loss: 0.8491, Time: 0.73 seconds\n",
            "Batch 365/940, Loss: 0.2524, Time: 0.72 seconds\n",
            "Batch 366/940, Loss: 0.4157, Time: 0.74 seconds\n",
            "Batch 367/940, Loss: 0.5664, Time: 0.74 seconds\n",
            "Batch 368/940, Loss: 0.4707, Time: 0.75 seconds\n",
            "Batch 369/940, Loss: 1.3400, Time: 0.75 seconds\n",
            "Batch 370/940, Loss: 0.6372, Time: 0.74 seconds\n",
            "Batch 371/940, Loss: 0.5635, Time: 0.74 seconds\n",
            "Batch 372/940, Loss: 0.5697, Time: 0.72 seconds\n",
            "Batch 373/940, Loss: 1.2998, Time: 0.73 seconds\n",
            "Batch 374/940, Loss: 0.7606, Time: 0.74 seconds\n",
            "Batch 375/940, Loss: 1.1002, Time: 0.74 seconds\n",
            "Batch 376/940, Loss: 1.0061, Time: 0.71 seconds\n",
            "Batch 377/940, Loss: 0.4489, Time: 0.71 seconds\n",
            "Batch 378/940, Loss: 0.9804, Time: 0.74 seconds\n",
            "Batch 379/940, Loss: 0.4970, Time: 0.72 seconds\n",
            "Batch 380/940, Loss: 0.8349, Time: 0.74 seconds\n",
            "Batch 381/940, Loss: 0.5969, Time: 0.75 seconds\n",
            "Batch 382/940, Loss: 0.6600, Time: 0.73 seconds\n",
            "Batch 383/940, Loss: 0.9627, Time: 0.75 seconds\n",
            "Batch 384/940, Loss: 1.0317, Time: 0.75 seconds\n",
            "Batch 385/940, Loss: 0.7958, Time: 0.74 seconds\n",
            "Batch 386/940, Loss: 1.0844, Time: 0.73 seconds\n",
            "Batch 387/940, Loss: 1.2564, Time: 0.74 seconds\n",
            "Batch 388/940, Loss: 0.3101, Time: 0.71 seconds\n",
            "Batch 389/940, Loss: 0.5855, Time: 0.72 seconds\n",
            "Batch 390/940, Loss: 0.7940, Time: 0.72 seconds\n",
            "Batch 391/940, Loss: 0.5790, Time: 0.75 seconds\n",
            "Batch 392/940, Loss: 0.4538, Time: 0.73 seconds\n",
            "Batch 393/940, Loss: 1.2720, Time: 0.71 seconds\n",
            "Batch 394/940, Loss: 0.5950, Time: 0.73 seconds\n",
            "Batch 395/940, Loss: 0.4632, Time: 0.73 seconds\n",
            "Batch 396/940, Loss: 0.5133, Time: 0.72 seconds\n",
            "Batch 397/940, Loss: 0.6431, Time: 0.71 seconds\n",
            "Batch 398/940, Loss: 0.6513, Time: 0.75 seconds\n",
            "Batch 399/940, Loss: 0.5002, Time: 0.75 seconds\n",
            "Batch 400/940, Loss: 0.7345, Time: 0.75 seconds\n",
            "Batch 401/940, Loss: 0.4940, Time: 0.74 seconds\n",
            "Batch 402/940, Loss: 0.9390, Time: 0.74 seconds\n",
            "Batch 403/940, Loss: 0.5062, Time: 0.75 seconds\n",
            "Batch 404/940, Loss: 0.4854, Time: 0.71 seconds\n",
            "Batch 405/940, Loss: 0.3748, Time: 0.73 seconds\n",
            "Batch 406/940, Loss: 0.6249, Time: 0.75 seconds\n",
            "Batch 407/940, Loss: 0.5423, Time: 0.74 seconds\n",
            "Batch 408/940, Loss: 0.9996, Time: 0.72 seconds\n",
            "Batch 409/940, Loss: 0.9588, Time: 0.72 seconds\n",
            "Batch 410/940, Loss: 0.5607, Time: 0.72 seconds\n",
            "Batch 411/940, Loss: 0.7120, Time: 0.74 seconds\n",
            "Batch 412/940, Loss: 0.5774, Time: 0.75 seconds\n",
            "Batch 413/940, Loss: 0.7606, Time: 0.74 seconds\n",
            "Batch 414/940, Loss: 0.6427, Time: 0.74 seconds\n",
            "Batch 415/940, Loss: 0.5774, Time: 0.73 seconds\n",
            "Batch 416/940, Loss: 0.5611, Time: 0.71 seconds\n",
            "Batch 417/940, Loss: 0.6306, Time: 0.71 seconds\n",
            "Batch 418/940, Loss: 0.5782, Time: 0.72 seconds\n",
            "Batch 419/940, Loss: 0.5907, Time: 0.71 seconds\n",
            "Batch 420/940, Loss: 0.6718, Time: 0.72 seconds\n",
            "Batch 421/940, Loss: 0.3441, Time: 0.73 seconds\n",
            "Batch 422/940, Loss: 0.3687, Time: 0.73 seconds\n",
            "Batch 423/940, Loss: 0.5466, Time: 0.72 seconds\n",
            "Batch 424/940, Loss: 0.3658, Time: 0.73 seconds\n",
            "Batch 425/940, Loss: 0.4132, Time: 0.73 seconds\n",
            "Batch 426/940, Loss: 0.6909, Time: 0.73 seconds\n",
            "Batch 427/940, Loss: 1.4136, Time: 0.73 seconds\n",
            "Batch 428/940, Loss: 0.3644, Time: 0.72 seconds\n",
            "Batch 429/940, Loss: 0.4691, Time: 0.71 seconds\n",
            "Batch 430/940, Loss: 0.5150, Time: 0.71 seconds\n",
            "Batch 431/940, Loss: 0.4015, Time: 0.71 seconds\n",
            "Batch 432/940, Loss: 0.5453, Time: 0.71 seconds\n",
            "Batch 433/940, Loss: 0.4442, Time: 0.75 seconds\n",
            "Batch 434/940, Loss: 0.2780, Time: 0.72 seconds\n",
            "Batch 435/940, Loss: 0.5606, Time: 0.71 seconds\n",
            "Batch 436/940, Loss: 0.5248, Time: 0.73 seconds\n",
            "Batch 437/940, Loss: 0.7699, Time: 0.72 seconds\n",
            "Batch 438/940, Loss: 0.1875, Time: 0.75 seconds\n",
            "Batch 439/940, Loss: 0.4756, Time: 0.73 seconds\n",
            "Batch 440/940, Loss: 0.2959, Time: 0.72 seconds\n",
            "Batch 441/940, Loss: 0.2013, Time: 0.72 seconds\n",
            "Batch 442/940, Loss: 0.8979, Time: 0.72 seconds\n",
            "Batch 443/940, Loss: 0.5865, Time: 0.73 seconds\n",
            "Batch 444/940, Loss: 0.9530, Time: 0.72 seconds\n",
            "Batch 445/940, Loss: 0.8955, Time: 0.72 seconds\n",
            "Batch 446/940, Loss: 0.6086, Time: 0.71 seconds\n",
            "Batch 447/940, Loss: 0.4585, Time: 0.73 seconds\n",
            "Batch 448/940, Loss: 0.4374, Time: 0.72 seconds\n",
            "Batch 449/940, Loss: 0.5214, Time: 0.72 seconds\n",
            "Batch 450/940, Loss: 0.5162, Time: 0.73 seconds\n",
            "Batch 451/940, Loss: 1.0776, Time: 0.75 seconds\n",
            "Batch 452/940, Loss: 0.3770, Time: 0.75 seconds\n",
            "Batch 453/940, Loss: 0.5345, Time: 0.75 seconds\n",
            "Batch 454/940, Loss: 0.5591, Time: 0.74 seconds\n",
            "Batch 455/940, Loss: 0.9679, Time: 0.76 seconds\n",
            "Batch 456/940, Loss: 0.6779, Time: 0.73 seconds\n",
            "Batch 457/940, Loss: 0.7866, Time: 0.73 seconds\n",
            "Batch 458/940, Loss: 0.8573, Time: 0.74 seconds\n",
            "Batch 459/940, Loss: 0.2201, Time: 0.74 seconds\n",
            "Batch 460/940, Loss: 0.8552, Time: 0.73 seconds\n",
            "Batch 461/940, Loss: 0.6533, Time: 0.73 seconds\n",
            "Batch 462/940, Loss: 0.5636, Time: 0.74 seconds\n",
            "Batch 463/940, Loss: 0.5036, Time: 0.73 seconds\n",
            "Batch 464/940, Loss: 0.4778, Time: 0.74 seconds\n",
            "Batch 465/940, Loss: 0.4952, Time: 0.74 seconds\n",
            "Batch 466/940, Loss: 0.7713, Time: 0.71 seconds\n",
            "Batch 467/940, Loss: 0.4320, Time: 0.72 seconds\n",
            "Batch 468/940, Loss: 0.7583, Time: 0.74 seconds\n",
            "Batch 469/940, Loss: 0.4534, Time: 0.71 seconds\n",
            "Batch 470/940, Loss: 0.6613, Time: 0.73 seconds\n",
            "Batch 471/940, Loss: 0.3310, Time: 0.71 seconds\n",
            "Batch 472/940, Loss: 0.6665, Time: 0.72 seconds\n",
            "Batch 473/940, Loss: 0.5262, Time: 0.73 seconds\n",
            "Batch 474/940, Loss: 0.6455, Time: 0.73 seconds\n",
            "Batch 475/940, Loss: 0.5712, Time: 0.72 seconds\n",
            "Batch 476/940, Loss: 0.3594, Time: 0.72 seconds\n",
            "Batch 477/940, Loss: 0.4572, Time: 0.72 seconds\n",
            "Batch 478/940, Loss: 0.6000, Time: 0.71 seconds\n",
            "Batch 479/940, Loss: 0.4129, Time: 0.72 seconds\n",
            "Batch 480/940, Loss: 1.0447, Time: 0.73 seconds\n",
            "Batch 481/940, Loss: 0.6950, Time: 0.72 seconds\n",
            "Batch 482/940, Loss: 0.8558, Time: 0.71 seconds\n",
            "Batch 483/940, Loss: 1.0694, Time: 0.72 seconds\n",
            "Batch 484/940, Loss: 1.0494, Time: 0.72 seconds\n",
            "Batch 485/940, Loss: 0.7355, Time: 0.72 seconds\n",
            "Batch 486/940, Loss: 0.1708, Time: 0.71 seconds\n",
            "Batch 487/940, Loss: 0.2711, Time: 0.70 seconds\n",
            "Batch 488/940, Loss: 0.4285, Time: 0.71 seconds\n",
            "Batch 489/940, Loss: 0.6602, Time: 0.72 seconds\n",
            "Batch 490/940, Loss: 0.3790, Time: 0.74 seconds\n",
            "Batch 491/940, Loss: 0.8523, Time: 0.74 seconds\n",
            "Batch 492/940, Loss: 0.5836, Time: 0.71 seconds\n",
            "Batch 493/940, Loss: 0.6941, Time: 0.74 seconds\n",
            "Batch 494/940, Loss: 0.4219, Time: 0.71 seconds\n",
            "Batch 495/940, Loss: 1.0194, Time: 0.72 seconds\n",
            "Batch 496/940, Loss: 1.0342, Time: 0.72 seconds\n",
            "Batch 497/940, Loss: 0.3062, Time: 0.74 seconds\n",
            "Batch 498/940, Loss: 0.5688, Time: 0.74 seconds\n",
            "Batch 499/940, Loss: 1.0686, Time: 0.72 seconds\n",
            "Batch 500/940, Loss: 0.6967, Time: 0.73 seconds\n",
            "Batch 501/940, Loss: 0.6480, Time: 0.71 seconds\n",
            "Batch 502/940, Loss: 0.5116, Time: 0.74 seconds\n",
            "Batch 503/940, Loss: 0.2115, Time: 0.74 seconds\n",
            "Batch 504/940, Loss: 0.4713, Time: 0.73 seconds\n",
            "Batch 505/940, Loss: 0.5832, Time: 0.72 seconds\n",
            "Batch 506/940, Loss: 0.4977, Time: 0.75 seconds\n",
            "Batch 507/940, Loss: 0.5522, Time: 0.75 seconds\n",
            "Batch 508/940, Loss: 0.4959, Time: 0.74 seconds\n",
            "Batch 509/940, Loss: 0.3416, Time: 0.75 seconds\n",
            "Batch 510/940, Loss: 0.2728, Time: 0.75 seconds\n",
            "Batch 511/940, Loss: 0.6640, Time: 0.73 seconds\n",
            "Batch 512/940, Loss: 0.2671, Time: 0.75 seconds\n",
            "Batch 513/940, Loss: 1.8908, Time: 0.74 seconds\n",
            "Batch 514/940, Loss: 0.4786, Time: 0.71 seconds\n",
            "Batch 515/940, Loss: 0.4651, Time: 0.73 seconds\n",
            "Batch 516/940, Loss: 0.5385, Time: 0.74 seconds\n",
            "Batch 517/940, Loss: 0.7714, Time: 0.73 seconds\n",
            "Batch 518/940, Loss: 0.3944, Time: 0.72 seconds\n",
            "Batch 519/940, Loss: 0.4586, Time: 0.72 seconds\n",
            "Batch 520/940, Loss: 0.5445, Time: 0.72 seconds\n",
            "Batch 521/940, Loss: 0.8268, Time: 0.74 seconds\n",
            "Batch 522/940, Loss: 0.4118, Time: 0.74 seconds\n",
            "Batch 523/940, Loss: 0.6506, Time: 0.72 seconds\n",
            "Batch 524/940, Loss: 0.4790, Time: 0.73 seconds\n",
            "Batch 525/940, Loss: 0.4275, Time: 0.71 seconds\n",
            "Batch 526/940, Loss: 0.3005, Time: 0.73 seconds\n",
            "Batch 527/940, Loss: 0.6499, Time: 0.73 seconds\n",
            "Batch 528/940, Loss: 0.7616, Time: 0.75 seconds\n",
            "Batch 529/940, Loss: 0.6891, Time: 0.73 seconds\n",
            "Batch 530/940, Loss: 0.8491, Time: 0.73 seconds\n",
            "Batch 531/940, Loss: 1.0610, Time: 0.73 seconds\n",
            "Batch 532/940, Loss: 0.7367, Time: 0.73 seconds\n",
            "Batch 533/940, Loss: 0.7333, Time: 0.74 seconds\n",
            "Batch 534/940, Loss: 0.6172, Time: 0.73 seconds\n",
            "Batch 535/940, Loss: 0.3726, Time: 0.72 seconds\n",
            "Batch 536/940, Loss: 0.7040, Time: 0.73 seconds\n",
            "Batch 537/940, Loss: 0.5530, Time: 0.71 seconds\n",
            "Batch 538/940, Loss: 0.9838, Time: 0.74 seconds\n",
            "Batch 539/940, Loss: 0.5629, Time: 0.75 seconds\n",
            "Batch 540/940, Loss: 0.5242, Time: 0.75 seconds\n",
            "Batch 541/940, Loss: 0.3267, Time: 0.75 seconds\n",
            "Batch 542/940, Loss: 0.6533, Time: 0.75 seconds\n",
            "Batch 543/940, Loss: 0.5443, Time: 0.74 seconds\n",
            "Batch 544/940, Loss: 0.3059, Time: 0.74 seconds\n",
            "Batch 545/940, Loss: 0.5679, Time: 0.73 seconds\n",
            "Batch 546/940, Loss: 0.9021, Time: 0.74 seconds\n",
            "Batch 547/940, Loss: 0.8157, Time: 0.72 seconds\n",
            "Batch 548/940, Loss: 0.4780, Time: 0.74 seconds\n",
            "Batch 549/940, Loss: 0.8007, Time: 0.72 seconds\n",
            "Batch 550/940, Loss: 0.3141, Time: 0.72 seconds\n",
            "Batch 551/940, Loss: 1.4297, Time: 0.71 seconds\n",
            "Batch 552/940, Loss: 0.4878, Time: 0.72 seconds\n",
            "Batch 553/940, Loss: 1.1639, Time: 0.75 seconds\n",
            "Batch 554/940, Loss: 0.7208, Time: 0.75 seconds\n",
            "Batch 555/940, Loss: 0.4170, Time: 0.74 seconds\n",
            "Batch 556/940, Loss: 0.3007, Time: 0.74 seconds\n",
            "Batch 557/940, Loss: 0.9745, Time: 0.71 seconds\n",
            "Batch 558/940, Loss: 0.4296, Time: 0.72 seconds\n",
            "Batch 559/940, Loss: 0.5971, Time: 0.74 seconds\n",
            "Batch 560/940, Loss: 0.6579, Time: 0.74 seconds\n",
            "Batch 561/940, Loss: 0.4789, Time: 0.75 seconds\n",
            "Batch 562/940, Loss: 0.6547, Time: 0.74 seconds\n",
            "Batch 563/940, Loss: 0.9056, Time: 0.74 seconds\n",
            "Batch 564/940, Loss: 0.6781, Time: 0.74 seconds\n",
            "Batch 565/940, Loss: 0.6882, Time: 0.71 seconds\n",
            "Batch 566/940, Loss: 0.4844, Time: 0.72 seconds\n",
            "Batch 567/940, Loss: 0.4371, Time: 0.74 seconds\n",
            "Batch 568/940, Loss: 1.0469, Time: 0.72 seconds\n",
            "Batch 569/940, Loss: 0.4179, Time: 0.74 seconds\n",
            "Batch 570/940, Loss: 0.5785, Time: 0.75 seconds\n",
            "Batch 571/940, Loss: 0.1469, Time: 0.74 seconds\n",
            "Batch 572/940, Loss: 0.4402, Time: 0.74 seconds\n",
            "Batch 573/940, Loss: 0.5845, Time: 0.73 seconds\n",
            "Batch 574/940, Loss: 0.7310, Time: 0.73 seconds\n",
            "Batch 575/940, Loss: 0.5951, Time: 0.74 seconds\n",
            "Batch 576/940, Loss: 0.9071, Time: 0.74 seconds\n",
            "Batch 577/940, Loss: 0.3784, Time: 0.73 seconds\n",
            "Batch 578/940, Loss: 0.9810, Time: 0.73 seconds\n",
            "Batch 579/940, Loss: 0.4887, Time: 0.71 seconds\n",
            "Batch 580/940, Loss: 1.3685, Time: 0.75 seconds\n",
            "Batch 581/940, Loss: 0.6129, Time: 0.74 seconds\n",
            "Batch 582/940, Loss: 0.3280, Time: 0.75 seconds\n",
            "Batch 583/940, Loss: 0.6984, Time: 0.72 seconds\n",
            "Batch 584/940, Loss: 0.7095, Time: 0.73 seconds\n",
            "Batch 585/940, Loss: 0.7699, Time: 0.72 seconds\n",
            "Batch 586/940, Loss: 0.7153, Time: 0.73 seconds\n",
            "Batch 587/940, Loss: 1.0103, Time: 0.72 seconds\n",
            "Batch 588/940, Loss: 0.7236, Time: 0.71 seconds\n",
            "Batch 589/940, Loss: 0.6594, Time: 0.74 seconds\n",
            "Batch 590/940, Loss: 0.5940, Time: 0.75 seconds\n",
            "Batch 591/940, Loss: 0.3567, Time: 0.74 seconds\n",
            "Batch 592/940, Loss: 0.4465, Time: 0.73 seconds\n",
            "Batch 593/940, Loss: 0.4088, Time: 0.73 seconds\n",
            "Batch 594/940, Loss: 0.6958, Time: 0.73 seconds\n",
            "Batch 595/940, Loss: 0.7449, Time: 0.71 seconds\n",
            "Batch 596/940, Loss: 0.4730, Time: 0.71 seconds\n",
            "Batch 597/940, Loss: 0.6563, Time: 0.75 seconds\n",
            "Batch 598/940, Loss: 0.9089, Time: 0.75 seconds\n",
            "Batch 599/940, Loss: 0.3187, Time: 0.73 seconds\n",
            "Batch 600/940, Loss: 0.3377, Time: 0.74 seconds\n",
            "Batch 601/940, Loss: 0.4309, Time: 0.74 seconds\n",
            "Batch 602/940, Loss: 0.5756, Time: 0.74 seconds\n",
            "Batch 603/940, Loss: 0.3612, Time: 0.75 seconds\n",
            "Batch 604/940, Loss: 0.1619, Time: 0.73 seconds\n",
            "Batch 605/940, Loss: 0.4483, Time: 0.71 seconds\n",
            "Batch 606/940, Loss: 0.6749, Time: 0.71 seconds\n",
            "Batch 607/940, Loss: 0.6675, Time: 0.74 seconds\n",
            "Batch 608/940, Loss: 0.4466, Time: 0.74 seconds\n",
            "Batch 609/940, Loss: 0.6315, Time: 0.73 seconds\n",
            "Batch 610/940, Loss: 0.3027, Time: 0.75 seconds\n",
            "Batch 611/940, Loss: 0.2708, Time: 0.72 seconds\n",
            "Batch 612/940, Loss: 0.5159, Time: 0.72 seconds\n",
            "Batch 613/940, Loss: 0.8209, Time: 0.72 seconds\n",
            "Batch 614/940, Loss: 0.6364, Time: 0.72 seconds\n",
            "Batch 615/940, Loss: 0.5878, Time: 0.72 seconds\n",
            "Batch 616/940, Loss: 0.5634, Time: 0.72 seconds\n",
            "Batch 617/940, Loss: 0.4286, Time: 0.74 seconds\n",
            "Batch 618/940, Loss: 0.4236, Time: 0.74 seconds\n",
            "Batch 619/940, Loss: 0.3516, Time: 0.71 seconds\n",
            "Batch 620/940, Loss: 0.6550, Time: 0.72 seconds\n",
            "Batch 621/940, Loss: 0.3704, Time: 0.71 seconds\n",
            "Batch 622/940, Loss: 0.4958, Time: 0.73 seconds\n",
            "Batch 623/940, Loss: 0.4412, Time: 0.74 seconds\n",
            "Batch 624/940, Loss: 0.3660, Time: 0.75 seconds\n",
            "Batch 625/940, Loss: 0.4716, Time: 0.73 seconds\n",
            "Batch 626/940, Loss: 0.2866, Time: 0.74 seconds\n",
            "Batch 627/940, Loss: 0.6865, Time: 0.74 seconds\n",
            "Batch 628/940, Loss: 0.5822, Time: 0.72 seconds\n",
            "Batch 629/940, Loss: 0.9968, Time: 0.72 seconds\n",
            "Batch 630/940, Loss: 0.8828, Time: 0.71 seconds\n",
            "Batch 631/940, Loss: 0.5648, Time: 0.74 seconds\n",
            "Batch 632/940, Loss: 1.0422, Time: 0.75 seconds\n",
            "Batch 633/940, Loss: 0.4938, Time: 0.73 seconds\n",
            "Batch 634/940, Loss: 0.8300, Time: 0.72 seconds\n",
            "Batch 635/940, Loss: 0.5634, Time: 0.72 seconds\n",
            "Batch 636/940, Loss: 0.2882, Time: 0.74 seconds\n",
            "Batch 637/940, Loss: 0.4656, Time: 0.73 seconds\n",
            "Batch 638/940, Loss: 0.3601, Time: 0.73 seconds\n",
            "Batch 639/940, Loss: 0.3625, Time: 0.72 seconds\n",
            "Batch 640/940, Loss: 0.6092, Time: 0.71 seconds\n",
            "Batch 641/940, Loss: 0.6840, Time: 0.71 seconds\n",
            "Batch 642/940, Loss: 0.4321, Time: 0.72 seconds\n",
            "Batch 643/940, Loss: 0.5608, Time: 0.71 seconds\n",
            "Batch 644/940, Loss: 0.8107, Time: 0.71 seconds\n",
            "Batch 645/940, Loss: 0.3767, Time: 0.73 seconds\n",
            "Batch 646/940, Loss: 0.7227, Time: 0.75 seconds\n",
            "Batch 647/940, Loss: 0.3613, Time: 0.72 seconds\n",
            "Batch 648/940, Loss: 0.2355, Time: 0.73 seconds\n",
            "Batch 649/940, Loss: 0.4251, Time: 0.74 seconds\n",
            "Batch 650/940, Loss: 1.1816, Time: 0.72 seconds\n",
            "Batch 651/940, Loss: 0.3379, Time: 0.73 seconds\n",
            "Batch 652/940, Loss: 0.3499, Time: 0.75 seconds\n",
            "Batch 653/940, Loss: 0.6793, Time: 0.75 seconds\n",
            "Batch 654/940, Loss: 0.4164, Time: 0.75 seconds\n",
            "Batch 655/940, Loss: 1.4077, Time: 0.73 seconds\n",
            "Batch 656/940, Loss: 0.8625, Time: 0.72 seconds\n",
            "Batch 657/940, Loss: 0.9578, Time: 0.73 seconds\n",
            "Batch 658/940, Loss: 0.7817, Time: 0.72 seconds\n",
            "Batch 659/940, Loss: 0.7257, Time: 0.70 seconds\n",
            "Batch 660/940, Loss: 0.5138, Time: 0.72 seconds\n",
            "Batch 661/940, Loss: 1.1240, Time: 0.74 seconds\n",
            "Batch 662/940, Loss: 0.4491, Time: 0.72 seconds\n",
            "Batch 663/940, Loss: 0.4413, Time: 0.75 seconds\n",
            "Batch 664/940, Loss: 0.7028, Time: 0.75 seconds\n",
            "Batch 665/940, Loss: 1.1244, Time: 0.74 seconds\n",
            "Batch 666/940, Loss: 0.3179, Time: 0.74 seconds\n",
            "Batch 667/940, Loss: 0.3978, Time: 0.74 seconds\n",
            "Batch 668/940, Loss: 0.5195, Time: 0.73 seconds\n",
            "Batch 669/940, Loss: 0.4746, Time: 0.73 seconds\n",
            "Batch 670/940, Loss: 0.6523, Time: 0.72 seconds\n",
            "Batch 671/940, Loss: 0.5859, Time: 0.73 seconds\n",
            "Batch 672/940, Loss: 1.1614, Time: 0.72 seconds\n",
            "Batch 673/940, Loss: 0.5654, Time: 0.75 seconds\n",
            "Batch 674/940, Loss: 0.5363, Time: 0.75 seconds\n",
            "Batch 675/940, Loss: 0.3520, Time: 0.73 seconds\n",
            "Batch 676/940, Loss: 0.8791, Time: 0.70 seconds\n",
            "Batch 677/940, Loss: 1.4088, Time: 0.71 seconds\n",
            "Batch 678/940, Loss: 0.5827, Time: 0.71 seconds\n",
            "Batch 679/940, Loss: 0.9007, Time: 0.75 seconds\n",
            "Batch 680/940, Loss: 0.7984, Time: 0.72 seconds\n",
            "Batch 681/940, Loss: 0.9051, Time: 0.72 seconds\n",
            "Batch 682/940, Loss: 0.7389, Time: 0.75 seconds\n",
            "Batch 683/940, Loss: 0.6901, Time: 0.72 seconds\n",
            "Batch 684/940, Loss: 0.8317, Time: 0.71 seconds\n",
            "Batch 685/940, Loss: 0.4331, Time: 0.73 seconds\n",
            "Batch 686/940, Loss: 0.4441, Time: 0.75 seconds\n",
            "Batch 687/940, Loss: 1.6160, Time: 0.73 seconds\n",
            "Batch 688/940, Loss: 0.2171, Time: 0.74 seconds\n",
            "Batch 689/940, Loss: 0.5186, Time: 0.75 seconds\n",
            "Batch 690/940, Loss: 0.6310, Time: 0.75 seconds\n",
            "Batch 691/940, Loss: 0.6021, Time: 0.73 seconds\n",
            "Batch 692/940, Loss: 0.3781, Time: 0.73 seconds\n",
            "Batch 693/940, Loss: 0.7559, Time: 0.73 seconds\n",
            "Batch 694/940, Loss: 0.5398, Time: 0.72 seconds\n",
            "Batch 695/940, Loss: 0.4488, Time: 0.73 seconds\n",
            "Batch 696/940, Loss: 0.3740, Time: 0.72 seconds\n",
            "Batch 697/940, Loss: 0.6224, Time: 0.75 seconds\n",
            "Batch 698/940, Loss: 0.3535, Time: 0.74 seconds\n",
            "Batch 699/940, Loss: 0.2777, Time: 0.75 seconds\n",
            "Batch 700/940, Loss: 0.4243, Time: 0.74 seconds\n",
            "Batch 701/940, Loss: 0.8414, Time: 0.74 seconds\n",
            "Batch 702/940, Loss: 1.0629, Time: 0.75 seconds\n",
            "Batch 703/940, Loss: 0.5275, Time: 0.73 seconds\n",
            "Batch 704/940, Loss: 0.3331, Time: 0.74 seconds\n",
            "Batch 705/940, Loss: 0.3744, Time: 0.71 seconds\n",
            "Batch 706/940, Loss: 0.5386, Time: 0.72 seconds\n",
            "Batch 707/940, Loss: 0.7820, Time: 0.75 seconds\n",
            "Batch 708/940, Loss: 1.1452, Time: 0.73 seconds\n",
            "Batch 709/940, Loss: 1.0237, Time: 0.72 seconds\n",
            "Batch 710/940, Loss: 0.5671, Time: 0.75 seconds\n",
            "Batch 711/940, Loss: 0.4275, Time: 0.74 seconds\n",
            "Batch 712/940, Loss: 0.2027, Time: 0.73 seconds\n",
            "Batch 713/940, Loss: 0.4390, Time: 0.74 seconds\n",
            "Batch 714/940, Loss: 0.7272, Time: 0.74 seconds\n",
            "Batch 715/940, Loss: 0.4610, Time: 0.74 seconds\n",
            "Batch 716/940, Loss: 0.5916, Time: 0.73 seconds\n",
            "Batch 717/940, Loss: 0.6000, Time: 0.72 seconds\n",
            "Batch 718/940, Loss: 0.5593, Time: 0.74 seconds\n",
            "Batch 719/940, Loss: 0.5131, Time: 0.75 seconds\n",
            "Batch 720/940, Loss: 0.3816, Time: 0.75 seconds\n",
            "Batch 721/940, Loss: 0.7258, Time: 0.72 seconds\n",
            "Batch 722/940, Loss: 0.1718, Time: 0.72 seconds\n",
            "Batch 723/940, Loss: 0.5766, Time: 0.72 seconds\n",
            "Batch 724/940, Loss: 0.2586, Time: 0.72 seconds\n",
            "Batch 725/940, Loss: 0.6772, Time: 0.71 seconds\n",
            "Batch 726/940, Loss: 0.6356, Time: 0.72 seconds\n",
            "Batch 727/940, Loss: 0.3348, Time: 0.72 seconds\n",
            "Batch 728/940, Loss: 0.8464, Time: 0.72 seconds\n",
            "Batch 729/940, Loss: 0.4445, Time: 0.74 seconds\n",
            "Batch 730/940, Loss: 1.4007, Time: 0.74 seconds\n",
            "Batch 731/940, Loss: 0.5529, Time: 0.74 seconds\n",
            "Batch 732/940, Loss: 0.3771, Time: 0.73 seconds\n",
            "Batch 733/940, Loss: 0.3417, Time: 0.71 seconds\n",
            "Batch 734/940, Loss: 0.6585, Time: 0.74 seconds\n",
            "Batch 735/940, Loss: 0.3499, Time: 0.75 seconds\n",
            "Batch 736/940, Loss: 0.5317, Time: 0.73 seconds\n",
            "Batch 737/940, Loss: 0.4956, Time: 0.74 seconds\n",
            "Batch 738/940, Loss: 0.4791, Time: 0.72 seconds\n",
            "Batch 739/940, Loss: 0.4631, Time: 0.71 seconds\n",
            "Batch 740/940, Loss: 0.2987, Time: 0.71 seconds\n",
            "Batch 741/940, Loss: 0.3740, Time: 0.73 seconds\n",
            "Batch 742/940, Loss: 1.1518, Time: 0.72 seconds\n",
            "Batch 743/940, Loss: 0.5607, Time: 0.71 seconds\n",
            "Batch 744/940, Loss: 0.4756, Time: 0.71 seconds\n",
            "Batch 745/940, Loss: 0.2312, Time: 0.71 seconds\n",
            "Batch 746/940, Loss: 0.3998, Time: 0.73 seconds\n",
            "Batch 747/940, Loss: 0.4589, Time: 0.74 seconds\n",
            "Batch 748/940, Loss: 0.5070, Time: 0.73 seconds\n",
            "Batch 749/940, Loss: 1.0641, Time: 0.73 seconds\n",
            "Batch 750/940, Loss: 0.4465, Time: 0.74 seconds\n",
            "Batch 751/940, Loss: 0.4287, Time: 0.71 seconds\n",
            "Batch 752/940, Loss: 0.5760, Time: 0.72 seconds\n",
            "Batch 753/940, Loss: 0.6548, Time: 0.73 seconds\n",
            "Batch 754/940, Loss: 0.6303, Time: 0.71 seconds\n",
            "Batch 755/940, Loss: 0.5524, Time: 0.72 seconds\n",
            "Batch 756/940, Loss: 0.7179, Time: 0.72 seconds\n",
            "Batch 757/940, Loss: 0.4140, Time: 0.71 seconds\n",
            "Batch 758/940, Loss: 1.0049, Time: 0.73 seconds\n",
            "Batch 759/940, Loss: 0.2805, Time: 0.73 seconds\n",
            "Batch 760/940, Loss: 0.7984, Time: 0.71 seconds\n",
            "Batch 761/940, Loss: 0.6797, Time: 0.73 seconds\n",
            "Batch 762/940, Loss: 0.4067, Time: 0.73 seconds\n",
            "Batch 763/940, Loss: 0.8561, Time: 0.75 seconds\n",
            "Batch 764/940, Loss: 0.5443, Time: 0.73 seconds\n",
            "Batch 765/940, Loss: 0.7487, Time: 0.73 seconds\n",
            "Batch 766/940, Loss: 0.4329, Time: 0.73 seconds\n",
            "Batch 767/940, Loss: 0.3614, Time: 0.73 seconds\n",
            "Batch 768/940, Loss: 0.6332, Time: 0.72 seconds\n",
            "Batch 769/940, Loss: 0.4532, Time: 0.74 seconds\n",
            "Batch 770/940, Loss: 1.6118, Time: 0.75 seconds\n",
            "Batch 771/940, Loss: 0.6533, Time: 0.73 seconds\n",
            "Batch 772/940, Loss: 0.3912, Time: 0.75 seconds\n",
            "Batch 773/940, Loss: 0.4971, Time: 0.73 seconds\n",
            "Batch 774/940, Loss: 0.8088, Time: 0.74 seconds\n",
            "Batch 775/940, Loss: 0.2214, Time: 0.74 seconds\n",
            "Batch 776/940, Loss: 0.3766, Time: 0.73 seconds\n",
            "Batch 777/940, Loss: 1.3504, Time: 0.73 seconds\n",
            "Batch 778/940, Loss: 0.2681, Time: 0.74 seconds\n",
            "Batch 779/940, Loss: 1.0984, Time: 0.74 seconds\n",
            "Batch 780/940, Loss: 0.2999, Time: 0.75 seconds\n",
            "Batch 781/940, Loss: 1.1824, Time: 0.74 seconds\n",
            "Batch 782/940, Loss: 0.8598, Time: 0.72 seconds\n",
            "Batch 783/940, Loss: 1.3027, Time: 0.72 seconds\n",
            "Batch 784/940, Loss: 0.9551, Time: 0.72 seconds\n",
            "Batch 785/940, Loss: 0.3954, Time: 0.74 seconds\n",
            "Batch 786/940, Loss: 0.7950, Time: 0.74 seconds\n",
            "Batch 787/940, Loss: 0.4074, Time: 0.72 seconds\n",
            "Batch 788/940, Loss: 0.5769, Time: 0.72 seconds\n",
            "Batch 789/940, Loss: 0.8558, Time: 0.74 seconds\n",
            "Batch 790/940, Loss: 0.2797, Time: 0.71 seconds\n",
            "Batch 791/940, Loss: 0.7392, Time: 0.71 seconds\n",
            "Batch 792/940, Loss: 0.7701, Time: 0.75 seconds\n",
            "Batch 793/940, Loss: 0.3656, Time: 0.70 seconds\n",
            "Batch 794/940, Loss: 1.0562, Time: 0.74 seconds\n",
            "Batch 795/940, Loss: 0.6386, Time: 0.73 seconds\n",
            "Batch 796/940, Loss: 0.3739, Time: 0.75 seconds\n",
            "Batch 797/940, Loss: 0.9327, Time: 0.75 seconds\n",
            "Batch 798/940, Loss: 0.4350, Time: 0.75 seconds\n",
            "Batch 799/940, Loss: 0.6711, Time: 0.74 seconds\n",
            "Batch 800/940, Loss: 0.6092, Time: 0.74 seconds\n",
            "Batch 801/940, Loss: 1.4955, Time: 0.75 seconds\n",
            "Batch 802/940, Loss: 0.2852, Time: 0.72 seconds\n",
            "Batch 803/940, Loss: 0.4102, Time: 0.73 seconds\n",
            "Batch 804/940, Loss: 0.7843, Time: 0.71 seconds\n",
            "Batch 805/940, Loss: 0.4367, Time: 0.74 seconds\n",
            "Batch 806/940, Loss: 0.2948, Time: 0.75 seconds\n",
            "Batch 807/940, Loss: 0.4927, Time: 0.74 seconds\n",
            "Batch 808/940, Loss: 0.4901, Time: 0.75 seconds\n",
            "Batch 809/940, Loss: 1.2274, Time: 0.73 seconds\n",
            "Batch 810/940, Loss: 0.8202, Time: 0.72 seconds\n",
            "Batch 811/940, Loss: 0.4405, Time: 0.75 seconds\n",
            "Batch 812/940, Loss: 1.0354, Time: 0.74 seconds\n",
            "Batch 813/940, Loss: 0.2797, Time: 0.71 seconds\n",
            "Batch 814/940, Loss: 0.3439, Time: 0.74 seconds\n",
            "Batch 815/940, Loss: 0.5800, Time: 0.75 seconds\n",
            "Batch 816/940, Loss: 0.2924, Time: 0.75 seconds\n",
            "Batch 817/940, Loss: 0.3566, Time: 0.72 seconds\n",
            "Batch 818/940, Loss: 0.4372, Time: 0.72 seconds\n",
            "Batch 819/940, Loss: 0.5393, Time: 0.73 seconds\n",
            "Batch 820/940, Loss: 0.4067, Time: 0.73 seconds\n",
            "Batch 821/940, Loss: 0.6385, Time: 0.74 seconds\n",
            "Batch 822/940, Loss: 0.4542, Time: 0.72 seconds\n",
            "Batch 823/940, Loss: 0.4901, Time: 0.73 seconds\n",
            "Batch 824/940, Loss: 0.2383, Time: 0.73 seconds\n",
            "Batch 825/940, Loss: 0.5871, Time: 0.73 seconds\n",
            "Batch 826/940, Loss: 0.3209, Time: 0.74 seconds\n",
            "Batch 827/940, Loss: 0.4028, Time: 0.74 seconds\n",
            "Batch 828/940, Loss: 0.3114, Time: 0.73 seconds\n",
            "Batch 829/940, Loss: 0.5292, Time: 0.74 seconds\n",
            "Batch 830/940, Loss: 0.6678, Time: 0.75 seconds\n",
            "Batch 831/940, Loss: 0.3972, Time: 0.71 seconds\n",
            "Batch 832/940, Loss: 0.5916, Time: 0.71 seconds\n",
            "Batch 833/940, Loss: 0.4749, Time: 0.72 seconds\n",
            "Batch 834/940, Loss: 0.3545, Time: 0.73 seconds\n",
            "Batch 835/940, Loss: 0.5064, Time: 0.70 seconds\n",
            "Batch 836/940, Loss: 0.2934, Time: 0.72 seconds\n",
            "Batch 837/940, Loss: 0.4004, Time: 0.73 seconds\n",
            "Batch 838/940, Loss: 0.7098, Time: 0.71 seconds\n",
            "Batch 839/940, Loss: 0.4367, Time: 0.72 seconds\n",
            "Batch 840/940, Loss: 0.8478, Time: 0.73 seconds\n",
            "Batch 841/940, Loss: 0.3507, Time: 0.74 seconds\n",
            "Batch 842/940, Loss: 0.6415, Time: 0.72 seconds\n",
            "Batch 843/940, Loss: 0.4503, Time: 0.73 seconds\n",
            "Batch 844/940, Loss: 0.6736, Time: 0.71 seconds\n",
            "Batch 845/940, Loss: 0.7806, Time: 0.75 seconds\n",
            "Batch 846/940, Loss: 0.3637, Time: 0.74 seconds\n",
            "Batch 847/940, Loss: 0.8022, Time: 0.71 seconds\n",
            "Batch 848/940, Loss: 0.3975, Time: 0.71 seconds\n",
            "Batch 849/940, Loss: 0.4326, Time: 0.73 seconds\n",
            "Batch 850/940, Loss: 0.1776, Time: 0.74 seconds\n",
            "Batch 851/940, Loss: 0.5403, Time: 0.71 seconds\n",
            "Batch 852/940, Loss: 0.5953, Time: 0.72 seconds\n",
            "Batch 853/940, Loss: 1.0013, Time: 0.74 seconds\n",
            "Batch 854/940, Loss: 0.6463, Time: 0.71 seconds\n",
            "Batch 855/940, Loss: 0.7028, Time: 0.72 seconds\n",
            "Batch 856/940, Loss: 0.5775, Time: 0.73 seconds\n",
            "Batch 857/940, Loss: 0.5427, Time: 0.72 seconds\n",
            "Batch 858/940, Loss: 0.2636, Time: 0.75 seconds\n",
            "Batch 859/940, Loss: 0.4009, Time: 0.73 seconds\n",
            "Batch 860/940, Loss: 0.3732, Time: 0.73 seconds\n",
            "Batch 861/940, Loss: 0.2214, Time: 0.73 seconds\n",
            "Batch 862/940, Loss: 0.3619, Time: 0.75 seconds\n",
            "Batch 863/940, Loss: 0.4227, Time: 0.72 seconds\n",
            "Batch 864/940, Loss: 0.5850, Time: 0.74 seconds\n",
            "Batch 865/940, Loss: 1.0235, Time: 0.75 seconds\n",
            "Batch 866/940, Loss: 1.0228, Time: 0.75 seconds\n",
            "Batch 867/940, Loss: 0.4135, Time: 0.75 seconds\n",
            "Batch 868/940, Loss: 0.4150, Time: 0.73 seconds\n",
            "Batch 869/940, Loss: 0.5550, Time: 0.73 seconds\n",
            "Batch 870/940, Loss: 0.3825, Time: 0.74 seconds\n",
            "Batch 871/940, Loss: 0.2380, Time: 0.72 seconds\n",
            "Batch 872/940, Loss: 0.3618, Time: 0.73 seconds\n",
            "Batch 873/940, Loss: 0.2893, Time: 0.72 seconds\n",
            "Batch 874/940, Loss: 0.2494, Time: 0.74 seconds\n",
            "Batch 875/940, Loss: 1.0299, Time: 0.74 seconds\n",
            "Batch 876/940, Loss: 0.4765, Time: 0.74 seconds\n",
            "Batch 877/940, Loss: 0.5727, Time: 0.74 seconds\n",
            "Batch 878/940, Loss: 0.3296, Time: 0.74 seconds\n",
            "Batch 879/940, Loss: 0.5106, Time: 0.74 seconds\n",
            "Batch 880/940, Loss: 0.7947, Time: 0.72 seconds\n",
            "Batch 881/940, Loss: 1.2599, Time: 0.72 seconds\n",
            "Batch 882/940, Loss: 0.5275, Time: 0.71 seconds\n",
            "Batch 883/940, Loss: 0.5419, Time: 0.72 seconds\n",
            "Batch 884/940, Loss: 0.3749, Time: 0.72 seconds\n",
            "Batch 885/940, Loss: 0.4549, Time: 0.73 seconds\n",
            "Batch 886/940, Loss: 0.6479, Time: 0.74 seconds\n",
            "Batch 887/940, Loss: 0.3205, Time: 0.75 seconds\n",
            "Batch 888/940, Loss: 1.1261, Time: 0.73 seconds\n",
            "Batch 889/940, Loss: 0.7251, Time: 0.74 seconds\n",
            "Batch 890/940, Loss: 0.4024, Time: 0.71 seconds\n",
            "Batch 891/940, Loss: 0.6497, Time: 0.72 seconds\n",
            "Batch 892/940, Loss: 0.5448, Time: 0.71 seconds\n",
            "Batch 893/940, Loss: 0.2821, Time: 0.72 seconds\n",
            "Batch 894/940, Loss: 0.4786, Time: 0.74 seconds\n",
            "Batch 895/940, Loss: 0.5491, Time: 0.73 seconds\n",
            "Batch 896/940, Loss: 0.2503, Time: 0.72 seconds\n",
            "Batch 897/940, Loss: 0.5610, Time: 0.72 seconds\n",
            "Batch 898/940, Loss: 0.5511, Time: 0.73 seconds\n",
            "Batch 899/940, Loss: 0.2646, Time: 0.72 seconds\n",
            "Batch 900/940, Loss: 0.4256, Time: 0.72 seconds\n",
            "Batch 901/940, Loss: 0.3743, Time: 0.72 seconds\n",
            "Batch 902/940, Loss: 0.4471, Time: 0.73 seconds\n",
            "Batch 903/940, Loss: 0.6908, Time: 0.73 seconds\n",
            "Batch 904/940, Loss: 0.3897, Time: 0.72 seconds\n",
            "Batch 905/940, Loss: 0.3679, Time: 0.74 seconds\n",
            "Batch 906/940, Loss: 0.3531, Time: 0.74 seconds\n",
            "Batch 907/940, Loss: 0.4176, Time: 0.73 seconds\n",
            "Batch 908/940, Loss: 0.4647, Time: 0.71 seconds\n",
            "Batch 909/940, Loss: 0.8071, Time: 0.73 seconds\n",
            "Batch 910/940, Loss: 0.5470, Time: 0.73 seconds\n",
            "Batch 911/940, Loss: 0.4781, Time: 0.71 seconds\n",
            "Batch 912/940, Loss: 0.7566, Time: 0.72 seconds\n",
            "Batch 913/940, Loss: 0.6535, Time: 0.72 seconds\n",
            "Batch 914/940, Loss: 0.4594, Time: 0.71 seconds\n",
            "Batch 915/940, Loss: 0.5966, Time: 0.71 seconds\n",
            "Batch 916/940, Loss: 0.4409, Time: 0.74 seconds\n",
            "Batch 917/940, Loss: 0.6225, Time: 0.73 seconds\n",
            "Batch 918/940, Loss: 0.3052, Time: 0.72 seconds\n",
            "Batch 919/940, Loss: 0.4504, Time: 0.74 seconds\n",
            "Batch 920/940, Loss: 0.8531, Time: 0.74 seconds\n",
            "Batch 921/940, Loss: 0.5660, Time: 0.73 seconds\n",
            "Batch 922/940, Loss: 0.3652, Time: 0.72 seconds\n",
            "Batch 923/940, Loss: 0.9724, Time: 0.74 seconds\n",
            "Batch 924/940, Loss: 0.2549, Time: 0.72 seconds\n",
            "Batch 925/940, Loss: 0.4964, Time: 0.72 seconds\n",
            "Batch 926/940, Loss: 0.2428, Time: 0.73 seconds\n",
            "Batch 927/940, Loss: 0.2530, Time: 0.72 seconds\n",
            "Batch 928/940, Loss: 0.5317, Time: 0.75 seconds\n",
            "Batch 929/940, Loss: 0.6484, Time: 0.74 seconds\n",
            "Batch 930/940, Loss: 0.4147, Time: 0.71 seconds\n",
            "Batch 931/940, Loss: 0.8212, Time: 0.74 seconds\n",
            "Batch 932/940, Loss: 1.1207, Time: 0.71 seconds\n",
            "Batch 933/940, Loss: 0.7799, Time: 0.71 seconds\n",
            "Batch 934/940, Loss: 1.0786, Time: 0.73 seconds\n",
            "Batch 935/940, Loss: 0.4560, Time: 0.73 seconds\n",
            "Batch 936/940, Loss: 0.6272, Time: 0.72 seconds\n",
            "Batch 937/940, Loss: 0.2827, Time: 0.72 seconds\n",
            "Batch 938/940, Loss: 1.0921, Time: 0.73 seconds\n",
            "Batch 939/940, Loss: 0.8547, Time: 0.75 seconds\n",
            "Batch 940/940, Loss: 0.5397, Time: 0.74 seconds\n",
            "Epoch Loss: 0.6973\n",
            "Epoch Time: 4198.39 seconds, Avg Time per Batch: 4.47 seconds\n",
            "Estimated Time Remaining: 69m 58s\n",
            "\n",
            "Epoch [2/2]\n",
            "Batch 1/940, Loss: 0.3520, Time: 0.72 seconds\n",
            "Batch 2/940, Loss: 0.3617, Time: 0.71 seconds\n",
            "Batch 3/940, Loss: 0.2542, Time: 0.74 seconds\n",
            "Batch 4/940, Loss: 0.3827, Time: 0.74 seconds\n",
            "Batch 5/940, Loss: 0.4596, Time: 0.74 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-26-0ee552ce2924>\", line 52, in <cell line: 52>\n",
            "    train(model, train_loader, criterion, optimizer, num_epochs=num_epochs, device=device)\n",
            "  File \"<ipython-input-26-0ee552ce2924>\", line 14, in train\n",
            "    for batch_idx, (images, masks) in enumerate(dataloader):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 757, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"<ipython-input-12-79fd1b719374>\", line 43, in __getitem__\n",
            "    image = Image.open(img_path).resize(self.target_size, Image.BICUBIC)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 2293, in resize\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 293, in load\n",
            "    n, err_code = decoder.decode(b)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 869, in getmodule\n",
            "    if ismodule(module) and hasattr(module, '__file__'):\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-0ee552ce2924>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-0ee552ce2924>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mbatch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-79fd1b719374>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEAREST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2293\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    }
  ]
}